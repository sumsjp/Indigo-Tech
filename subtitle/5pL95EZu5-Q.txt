大家好，我叫李厚明，我是棕榈资本的创始人
曾经是互联网的从业者，当时 Indigo 是我的老板
哈，没想到今天有一天可以跟我的老板不是开会的方法，而是对谈
非常的高兴，感觉到自己奋斗了 10 年，就是为了这一刻
对，厚明，你太谦虚
今天厚明邀请我来做一期对谈
因为我个人也做写博客，写了快两年了，写科技博客
对我自己稍稍介绍一下，因为我们音频会发在两个不同的渠道上面
今天是这样
今天算是 Indigo Talk
Indigo 有一款播客节目，更新非常非常慢
我们一般有话题才会去做一下，所以正好今天有幸也邀请到李厚明
我们一起来聊一下 AI 这个话题
对我自己的 background 前面也介绍了
我之前在微博跟厚明是同事，后来自己也做了一段时间早期投资
投的项目不多，但是应该算成功率蛮高的
现在在温哥华也做一些早期孵化，也是个人做这些
怎么说呢
科技博主要干的事情，可能像一个小科技媒体一样
所以我会有自己的博客，也有自己的播客，更新很少
也会写文章，也有课程类似的，会做这一些去学习一下前沿科技
所以我们这档节目也是为了普及前沿科技的知识
主要是面向投资人或者创业者这样一些群体的
帮助大家了解日新月异的这些变化
好吧，我先说一下为什么我当时特别想跟 Indigo 来一次对谈
哈，有几个原因
第一个，我本身是一个技术的门外汉，但是算是一个新技术的爱好者
在我刚开始接触到不管是 Web 3 还是 AIGC 概念的时候
你之前做的《机器之心》的课程对我启发特别的大
我推荐了失好几个，我的朋友都买你的课
对，尤其是你的第二讲，我觉得是起非常大的
尤其是里面讲到了 AI 发展的历史的部分
其实一开始的初心想邀请你给我们很多，对吧
可能我们大家听到了很多有趣的一些词汇
什么 ChatGPT，OpenAI，Dall-E，Stable Diffusion
所以我在想能不能有可能请你再简单地帮我们
回顾一下 AI to AIGC 的发展历史
OK，对，我课程里面讲过
因为今天我跟厚明做这一期对谈主要的目的
厚明说想面向他和他的朋友们是吧
以及我的一些受众人来再做一次科普工作
所以我们这一期节目不会聊到特别深入的基础话题
也不会聊到特别的 deep 的一些内容上去
我们尽可能的有逻辑脉络，清晰的把这些事情给讲解一遍
方便大家看清现在的发展走势
这是我自己之前写文章主要的目的，我们没有写特别深的技术文章
对，说到 AIGC 是吧
AIGC 这个词去年 2022 年初年中开始很流行的
因为 Dall-E 和 Stable Diffusion 这两个开始的
他差不多在去年 9 月份，ChatGPT 还没出现的时候，这两个火爆了
在我们这里，我记得当时我就录了节目，但是好像没什么关注
北美其实火爆了，我们七八月份开始的，去年开始关注的
在这个之前，其实算法生成， AI 生成没有那么好的效果
包括 Dall-E 的第一个版本之前还没有 Stable Diffusion
基本上所有的爆发都是 2022 年初开始的
对这一次爆发可能还是因为其实 GPT-3 在这之前就有了
对，在 2020 年中就有了
它也是属于 generative 的 AI
还是生成式 AI，但是他通过 API 的方式提供的
只有程序员可以用，对不对
Dall-E 就提供了用户界面的对吧
因为 OpenAI 的产品一直是产品做的好，产品做得很好
他们很简单使用，所以大众一进来他就会流行起来
但是如果你程序员使用，可能永远是在小圈子里面
这是爆发的第一个点
第二个也是因为算法，应该叫模型的变化
我们现在用的 AIGC 的模型基本上都是用的 Transformer
我在课程里面讲了叫变形金刚，Transformer 模型 2017 年诞生的
差不多进化了很多年
Google 有很多产品现在都是这个模型
但是 Google 只把这个模型用在自己改造自己的产品
什么 BERT 搜索引擎，或者它的一些库
在这个之后还是归功于 OpenAI，他们会做产品
他们把这个东西经过了两到三版的升级之后
就像 GPT-3 然后有了 Dall-E
还有后面的一些产品，导致概念就非常流行了
很多的 AI 的这些创业的公司都在做生成式 AI
生成图片，包括 Jasper 之前可以写文案的
在这之前其实都有很多公司
在 2021 年之后， 2021 年最后开始爆发的
我是因为去年 6 月份正好去了一趟美国
那个时候是第一次接触到 Jasper AI
当时在美国试用了一下产品以后
第一感觉我可能以后不是不用学英文了
他能够把英文写作的很多问题解决得非常好
沿着这个事情就关注到了 MidJourney、Stable diffusion
确实在国内这个问题的讨论可能会稍微晚一些
因为国内产品影响到我们的生活
大范围的影响还是在 ChatGPT 出来之后
前面在 MidJourney 大家可以使用的时候，也是让国内的创业者、投资人都兴奋了一波
但我感觉等大家真的觉得OK，这是一个不可阻挡的趋势
还是等到一个应用级的产品 ChatGPT 真正进入到了很多人的日常生活当中的时候
对，其实我有个小问题想问你一下
你看你刚刚提到模型的一个变化
Transformer 概念模型的诞生，其实很大程度上影响了 AIGC 的赛道的一个发展
但是这个文章其实 17 年就发了，为什么影响力是在最近几年才显性认出来
中间发生了什么
有待科研转成实用嘛，这个是需要时间的
2017 年发的是白皮书，哦是 Paper，发的是论文
我们之前搞 Web 3 搞多了，叫白皮书
对，发的是论文
之后发完之后我记得 2018 年很快 OpenAI 改方向马上就开始做 GPT 了
因为他们最早用的，OpenAI 是第一批用 transformer 模型把科研转换成工程的一个公司
他们很快，速度比 Google 速度还快，Google 只是后来才同期做了 BERT
我觉得做 AI 领域的这些研究者或者是这些公司都会用过 BERT， Google 的引擎 BERT，他做文字处理的
那个时候就做文字转换，我们一般叫自然语言处理
这 NLP 里面的，把一段语言输进去， translate 到另外一种语言
或者 translate 另外一种格式，他都是干的序列化处理
在这之后很快的速度，差不多 2018 年 19 年，那个时候还没有很现成的应用
但是可能很多公司都在用 BERT， Google 的实验型的它模型来做自己的训练
基本上都是在用它的，因为它是 open source 的
再往这之后就看到了 GPT-2 出来了，它模型规模很大
对大模型的竞赛就是他们带起来的，他拼命的训练他们自己的模型
再到 GPT-3 就 1700 亿个参数了
对，我们后面话题好像要聊到模型规模
对，OpenAI 把这个带上了快速进智能赛道Google，我记得 Google 也跟进了
其实 Nvidia 也有， Nvidia 一直和在和微软有合作，他们也在做，我忘图灵还是什么大模型
大规模模型的训练大概是这样子的，之后就开始大规模模型
之前我在课程里面讲到了用从模型，从算法变成了模型，是吧
从模型变成了大规模语言模型这样的一个进化
所以现在都被模型统一了
我们一上去分享模型，做模型，用别人的模型来做这东西，没有人提算法
对，我听说 GPT-1 和 2 的效果其实并没有和 NLP 原来的 NLP 算出来的效果差异那么大，对吧
是的， GPT-2 就比较一般
对，但是因为后来它 GPT-3 增长了 100 倍，参数 100 倍，大力出奇迹了
但是你看 BERT 的数量级还是在三亿多，我记得
BERT 是属于 GPT-2 的之前数量级的，但是 Google 还有其他的嘛
Google 因为 BERT 他们自己在内部使用，搜索调优用，还没有用到其他地方
其实我们刚刚提到了很多的词汇哈，我觉得可对于包括我
哪怕我关注赛道关注了一段时间了，我都有一个困惑
感觉价的 AIGC 里面大家提到的各种产品，各种词汇，或者是各种不同的模型，对吧
你能我用你的方法来做一下分类，帮助我们这种外行人稍微地更快地入门一些
你说模型吗
对模型或者是以产品这种做一些分类
产品分类很复杂，基本上，我想想看，现在大家都讨论的是模型
因为现在比较焦虑的是没有大模型，各个大厂都很焦虑为什么没有大模型
对，模型大我们在录之前我跟厚明还聊了怎么定义大模型
也不好通过参数定义，因为它可能通过算法会让参数变少
是的 对
其实我给的可能还是难度和训练规模，他用的语料的数量，规模和训练的成本
对，只要你极大的时候，语料极大和训练成本极高的时候，我们可以成为大模型
但这个也不好定义
按照 OpenAI 训练方法，至少是几个亿美金起
对，这样往上的训练成本，会上千万美金的
就在这里我插一下 OpenAI 是花了几个亿美基金，你觉得如果现在对吧
假设世界上的团队想复现一下 OpenAI 的训练结果
有没有可能因为他们要把路趟出来了，我们少走一点弯路
成本能够 downsize 到，比如 5000 万美金之类的
当然可以，现在有已经 down size 了
对，我看到 Ark 每年都有一个 big idea 的报告，我都建议大家也可以看一下
里面提到了很多很好的数据的分享
关于训练成本是每年 60% 的下降， OK 60%，或者现在他们预测是60%，实际上是75%
可以训练成本的摩尔定律
对，成本 cost 往往下低
训练成本的摩尔定律，比摩尔定律还要快
摩尔定律每 18 个月翻一番，它是每年要降 75%
OK 对，刚刚提到 collapse 会很快的
所以你刚刚提到了一个，可能是按照这大模型，按照训练难度对吧，和投入的时间前来做一个分类
可能会按照最后算出一个比例是吧
你最终要花这么多时间，这么多成本
或者以及训练多少内容，得出什么结果
这个东西可能成本没那么高，但是你需要的时间和研发投入的时间都是需要时间的，也没法衡量
每个公司都有自己的算法，因为现在也没有一个 benchmark 来定义什么是大模型
对，所以直接问问，你觉得 stable diffusion 算大模型吗
中等规模的吧，他算中等规模的模型
中等和大之间对这样子的一个比率
而且大模型有一个专门的英语的一个描述，叫做 large language model， LLM 是吧
large language model 这样的一个缩写叫大模型
大规模语言模型，特指是语言模型
但 Stable Diffusion 不是语言模型
对，它是一个图像
但是它也用了，用到了自然语言的一些混合的方式
最终它是一个 diffusion，diffusion 的这样一个图像算法来生成图像
它叫应该叫做图像生成式模型
其实你刚提到了一个点，哈，就是文字和图像
就是 stable diffusion，它是图像
刚刚我们讨论的，包括 Bert， GPT 它其实都是文字
文字图像从训练难度上来说差异大吗
看语料和规模
因为现在互联网上最容易得到的就是文字和图像，文字应该比图像还要多
对是，但是文字它会涉及到不同的语言，以及清洗这些文字难度我觉得会比清洗图像要大
因为图像在 computer vision 赛道上面，或者不叫赛道
这种科学的 AI 的图像的识别程度上，打标，自动打标记，很已经很成熟了
对，应该说训练起来会比文字要简单一些
所以你要大量的图像往里面喂，而且很多
你看 OpenAI，它训练 Clip， CLIP 把图像转成文字的，还有他自己做的 Dall-E 这一些东西
它其实都是基于语言模型来的，它把语言模型和图像映射起来，定义图像里面有什么
再做一个 Match，我们叫做，现在很流行讨论
又有一个新的名词叫多模型，两个模型合在一块，多模态或者叫多模态
MultiModel 对，我记得 Sam 在采访的时候，去年接受 Grey 采访的时候提到了的
未来的发展方向肯定是多模态的，因为你只靠语言模型
让文字转文字，你没法实现智能，你至少得让人看见，对吧
所以有就有了图像的文字之间的相互转换，再加上声音是吧
再加上很多视觉的东西或者更复杂的数据进去，不同的
它最终通过同一种模型算法，都可以用 transformer，它最后就会融合在一块，形成一个数据集
把很多数据形成一个数据集，这样就会产生这样的一个结果
其实 CLIP 会这样做的
你刚刚也提到了一个点，哈
最后是不是多模态的形，是不是形成一个数据集的
其实这个东西也是现在很多人在讨论的事情对吧
大模型在发展过程当中，现在文字跟图像的训练我觉得还没有完全的统一起来
大模型哈，进步得这么的快
大模型会不会吃掉所有的中模型，或者是将来吃掉应用层，这个事情你怎么看
吃掉中模型肯定是的，因为他
你看现在大家玩下 g p t，已经把很多翻译什么 Grammerly 都可替代了
对翻译工具 DeepL 那些表现对吧，人家就顺道把它给超了
对这样的一个结果，因为随着参数增加的时候，什么为所有研究 AGI
artificial general intelligent 就是通用的人工智能首选要研究语言
对，首选语言，我不知道国内团队是不是这样做的，反正硅谷的那些团队都是选择这个方向
如果你想做 AGI，必须从语言下手，因为我们至少是人类这样的生物
智能物种
我们表现出智力语言对不对
所以一个人的语言表达能力和他的智力直接相关
你所有的知识沉淀，集中逻辑思维，都是靠语言重新组织的，其他的只是素材而已
是的，你刚刚提到文字训练跟图像训练当中，哈，你提到的就是个标注，对吧
图像的标注更成熟，标注是伴随着这 large language model 来发展当中的
我其实也有一个观点，我觉得在图像生成和文字生成上，图像的生成它更加是无中生有
一个普通人，对吧
或者我作为他的用户，我也很难说他一定是对的，一定是错的
但是文字生成以后，你的语法是不是通顺
你是不是答案，你这个答案正不正确，我觉得更容易被用户感知到
这是我觉得可能
对，可能文字会对控制，对结果的控制要求的更高一些
需要更稳定一些
因为 AI 最大的算法输出转化出来就是不稳定
我们让他每次输出都稳定
对，不然就是胡言乱语了
你看到一段胡言乱语的文字和一个乱拼接的图片，肯定会觉得文字更可怕
对，完全读不懂，但是图片你看看还行
对，虽然我们都知道他是拼接的，但是他还能够看得懂一点意思
对，所以我记得上次你跟我提过 stable diffusion 的重点可能在 stable
对，在稳定上面对，要稳定输出才对
其实大家都是控制稳定性
对，要稳定持续的输出
我们再讨论一个问题
我们知道文字里面哈，文字里面也分多个语种
这中文和英文的训练难度上，你觉得这两个你怎么看
有一些人说英文的逻辑性更好，语料更多
中文可能天生我们语言上的语法逻辑性差一些，可能高质量的文本少一些，训练难度大一些
我觉得后面更严重一些，高质量文本
因为英文基本上高质量的作品，论文，或者一些好的表达方式，包括文章
大家看内容也知道，我平时在网上看好的优质的内容输出 content 要大大于中文的
对，这个是它有很好的语料
另外一个，本身英语逻辑结构要强一些，同样的语句，表达的歧义会少一些
简单一些，比中文比起来简单一些，比中文简单，所以它更适合于做表述
所以中文的训练可能难点在语言本身
语言本身还好，我觉得包括机器肯定可以玩得出语言本身的逻辑的，这个不在话下，而是在语料的问题
明白，短时间之内好像也没什么好的解决方案，对吧
对，没有你把中文，你最终只是把整个把英文翻成中文
最后怎么说全世界的语言
在 AI 的眼里面，全世界的语言都是英文，他已经统一了
对对，都是数字，最终都会表现成英文，因为都是数字对都token，都 token 对
他最后最终会表现成英文，再通过英文翻译成其他的语言
我感觉慢慢的随着 AI 在普及，进一步普及
实际上世界的全部人类加上机器的共同语言就是英文
其他的语言在他在这个世界里面就会慢慢没有，底层就没有了
而且这些语言可能会更像英文，它可能会像我们在民国时期把很多外来词翻译进来一样
我们没有这些词汇的，我们没有这些表述方法的
在中文里面，我把英文翻译过来，包括语序都是学英文的，我们是倒装句，都会慢慢英文化了
对，是，后来会只会越来越严重，这个事情不会
所以很多语言都会消失掉，但是中文人都要用的对，应该不会面临这么大的问题
得看一下国产大模型怎么做了
正好我们也稍微展开讨论一下
国内其实投大模型哈，大家 by default 它有一个机会叫关门的机会
比如这个可能我们也很难自由地 access to OpenAI 的 API
所以我们国内有这样的机会，你怎么看
这个事儿
肯定是有的，因为对大家都想得到互联网之前的发展的
对，因为一些限制，国内就有很多同等这样对应的产品，这个很正常的
但是这一次的限制会造成了一个困境，你在一个相对于过去的知识沉淀和语料素材很匮乏的情况下
有待他训练的东西再做出来之后，你是不是先天就有一些差距了
当然我们可以去拿英文来训练，对，但是拿英文训练来处理你
相当于永远是在追赶别人，你没有自己的积累的特点
所以这里面有我感觉有一个比较纠结的地方，我们如何能够因为大部分的语言模型的这种训练
其实 50% 多或者60%，更多的依赖于数据以及数据的结构，它怎么去分类，怎么去清洗它们
所以这个东西也我们用同样的算法，都是公开的
paper 是公开的，调优不公开对吧
这是核心，就跟做药一样是吧
我的佐料不公开，我的分子式告诉你的
对，我整个过程的做法流程我都不知道，你都不知道我怎么做的
为什么我们同样去吃什么感冒药，或者属于吃什么泰洛
北美的泰洛效果比中国国产好多了
一样的分子式，一样的配方
所以未来的我感觉语言模型训练出来也是像制药一样的
这个东西是很现实的，不要说骇人耸听，这是很现实的结果
就是差一点，但是不知道
可能差在数据上，可能差在佐料上面，可能差在一些方法上
我正好也有个比喻想跟你讨论一下
你刚刚药的比喻我觉得非常的好，浅显易懂
我其实想哈， transformer 那篇论文是不是有点像原子弹的原理
对， OpenAI 也好， Bert 也好，包像国内这些
不管是大公司还是创业想做大模型也好，其实都是工程师，本质上做的对，都是工程师
对，工程是个工程问题，它不是一个科研问题
对，但你刚刚悲观的点是OK，可能别人有更好的材料，对吧
但是从历史上来看，哈，我们中国的工程能力是极强的
就是一件事情，我们只要看到这个世界上有别人做出来了，我知道它的原理是什么
我觉得好像在中国的发展历史当中都做得还可以
是还可以 对，就是药除外
后面就是一个成本和代价的问题
对，我觉得这是一个问题
第二个问题也是一个问题，我觉得是更严重的问题
我感觉大家工程能力很好，中国工程师能力很好，非常棒
我们可以基本上可以都学过来，而且可能有些地方还超越他们原生的
对，预料其实也不是问题
这 OpenAI 能搞到英文语料，我们搞得到吗
一样搞得到 对吧
只是我们用纯中文的方式来做，对，可能会有问题，但是你还是得走到从英文再到中文的会好一些
这样会
因为其实现代世界现在文明是英文书写的，不是中文书写的
你用这些东西去回答中文去回答，很多问题都回答不了
这个道理懂的
是的，这是近代文明，是海洋文明
海洋文明吧
其实到这里就说到刚刚说的成本问题，对吧
成本里面其实有，我觉得分成两个成本哈
第一个是算力成本，典型的我们的卡，看有看是中国有多少张卡，都储备了多少算力
第二个人才先讨论这个卡的问题哈
我相信哈，国内很多大厂其实都储备了不少的芯片，储备了不少的GPU
我其实比较好奇的事情，这个事情在里面是多大的一个must
因为我理解就是创业公司如果做这件事情，它很难说有海量的资金来买
而且即使他想买，在现在的中美关系的情况下，他也不一定容易得到，对吧
所以你怎么看这个事
还是咱等时间，因为算力每年都在下降 75% 嘛
对，是的，这肯定可以等的
对
我觉得在这个里面就有很大创业机会，你把训练成本降低呀，也是很好的创业公司做的事情，对不对
降低训练成本应该有不少，硅谷应该很多公司在做，做训练技术架构
但是你说的还是一个硬伤问题，算力的真的是算力
OpenAI 愁的也是算力
我看 Sam 他采访的时候，他说我们缺电，缺电，缺人员，缺算力
对，这个是很现实的问题
一样的，他们也要花很多钱嘛
对，一个是需要更快的芯片，第二个是需要更好的技术结构的
变化，比如优化算法或者芯片，更适合来做这种计算
你比如 Tesla，我记得我在上一讲你就不知道听了没有
Tesla 自己做芯片，自己做 Dojo，自己来做机柜，自己来做所有的冷凝，自己来做电
他从挖矿开始，全部自己解决
对，虽然我觉得世界上很少有公司可以干这种事情
对，除了他不能做芯片之外是吧
这个是可能未来很多公司要面临的问题，特别是我们再把算法，比如模型推到下一步的时候
我们需要更多的数据，而且核心的数据是机器生成数据来训练自己
但是可能现在对于自然语言还好，因为我们的语料其实很多了，世界上的语料足够多了
我们不需要机器胡说，生产很多语言来自己骗自己了
但是如果要做到现实世界的训练，机器的感知语料就完全不够的
对，我们必须得去模拟现实，让机器在模拟中学习，要有大量的算力了
对，太可怕了
这个事情其实我们上次也讨论过，上次我们俩提到的时候讨论过
能够真正带我们到 AGI 的是更多算力，还是新的 transformer
还是新的逻辑
新的 paper
第一个是算力，算力是在最前面，算力也是
假如今天我们有无限算力了，是一定能带我们 AGI 吗
笨一点得方法就傻一点，就可以往这个方向去
但是第二点就是跟生物进化一样，我们总是有新的机制出现
有新的生物结构出现，演化一样，演化道理，它突然就有一个新的
比如 transformer 2.0 或者另外的名字出现了
它很快的算法就跟病毒一样，所有的都把它统一了
现在 2017 年到 19 年的 2017 年到 2020 年之间
这三年就跟在 AI 的生物界里面突然出现了一种新的生物结构一样
它有很多优势，它一个模型，它一个结构，就处理了图像、语音所有的数据
基本上所有的过去的研究我们都停下来，我们不干了，我们就用新模型来把这些事情重做一遍
这是过去，现在又到了这个时刻，而且我们发现什么 GPT-3 或者 ChatGPT 在应用层面
它更接近应用层，又可以干这个，又可以来一次
我们都不用干了，翻译也不用做了，什么什么摘要也不用做了，我们都用它再来一次
是，听到过一个做 AI 的朋友开玩笑，最开始他们 NLP 对吧
就是上一代做 NLP 的人，看到 transnormal 论文出来了以后，当觉得自己头都要秃了，要失业了
对他很有杀伤力，因为他靠关注
因为 NLP 最关键就是机器转码的时候，前言不搭后语，因为他不记得前面说了什么了
对，所以现在要看一个 token 的长度，每次输进去的，比如 4096 这样的一些长度
来让机器记住前面说什么，聊天的篇幅是有限得
人类，我们俩再聊一次节，这个节目我们聊一个小时，如果我们聊太久
我们聊 3 个小时，我真的不知道一个小时前我们说了什么
是我都已经忘记我们刚开始说了什么
对，所以我们大脑也在尝试去回忆之前的
但是人类有很多技巧，会记的人
他会把前面压缩一下是吧，存在一个地方去来结构化一下
人比智商就比这个嘛，我们谁的记忆力更强，记忆力更好，谁更会结构化的存储知识，智商就高是吧
会算嘛，会存嘛，跟 AI 是一样的逻辑，对 AI 也需要
我觉得可能接下来在模型上面可能会有一些更复合的模型
可能多模态会用在解决同一个问题上面
比如像 Facebook， Meta 就要做的这种，他们可以记住很长的文章，可以生成很长的文章
对，他可能会有一些技巧，这蒙特卡洛算法这种都可以融到 transformer 里面去
我现在能不能这么理解
现在的 large language model 其实在处理短逻辑上，我认为表现得还不错
因为我感觉在封闭对话里面，我跟 ChatGPT 的对话还是有很好的上下文的
但是可能让他变成是真正的长篇文章的续写，或者是给输出一大段话，一整篇文章是有一些难度的
对，有难度的，很有难度
写长篇小说，中篇小说的很有难度
对短篇可以
你有用 Notion AI 吗
用了，很早就用它了
它里面有个续写功能
很方便呀，我打一段话，我要写个什么待办事项，我要干个什么事情，你帮我把这个东西写出来
它就跟你 123456 列出来了
跟 ChatGPT 一样的应该
他后面就用了 GPT-3 的
应该是
我还问了 ChatGPT 这个问题，他说他不知道
所以他作为一个 AI language model，他不知道 notion AI 用的是用的什么
这次不知道，因为他训练已经结束了吗
那个时候
对，因为我
其实今天就我们俩讨论一些问题，我还让我的 notion 稍微续写了一下
你可以说一下，举个例子看看这个
比如我问他一个问题，我说你怎么看 Open AI 这个 API 的降价
对，他说降低了使用门槛，对吧
扩大了用户群和应用场景，说明了 GPT-3 已经基本成型
OpenAI 需要通过降价来争取更多的用户，获得更多的数据和反馈，来提高模型的泛化能力
3 同时也给后者带来了压力，要追赶 GPT-3 的技术优势，需要投入更多资源
我感觉这是 refresh 的一个
这还是稍微思考了一个人的文章
对，他把很多关于降价的经典的回答都整理了一遍，而且做了一个排序，好像
对哈哈哈，排序还比较有逻辑
听上去特别最后的通过降价控制市场，对，是他降了十倍，这样 1/ 10 的价格
然后这会让很多现有的已经低于现在很多语言翻译 API 的价格，调用的
所以直接用它来做翻译，不用之前的引擎的
它相当于把我们刚才说的，我们正在这一次转变之中，用以一个统一的方式把所有的问题解决掉了
对我们这个问题稍微展开一下讨论
你觉得这个事情对 Google 这种大公司，它其实也很有技术实力对他们的影响
影响是吧
是的，对
但所有的这些东西都是 Google 玩出来的，Google 最先玩出来的
Google 的 Google 更像是一个研究机构，它的 AI 很分散
对，他的很多小组里面， Google 不是一家 Google 的 AI 产品线，它不是像一个企业一样去推动的
它不是像 Sam 在里面
Sam Altman 他是一个企业家，他知道节奏
他知道什么时候把东西放出来，有市场效果
Google 没有 Google 他都分散在不同的研究部门里面，都是一群科学家在干这些事情，连产品经理都没有
都是这样子，一堆 API 出来，一堆程序员天天在用它，还用的很好，不错
Google 的技术肯定是储备足够了，来应对现在的竞争是一点问题没有
他的问题一个是大企业病，第二个是之前的模式太分散了，他没有形成合力
第三个， Google 的总是想用这些技术来优化自己的产品线，改善搜索，改善广告，改善 YouTube 的检索
对，他们为什么这样子
他这样子会更好卖广告是吧
不侵害自己现有的变现业务
对，直到微软的 CEO 说我反正不靠广告赚钱，我就把搜索成本降得很低
对对对对对
对对对，这一脚把他踹到了，他当天股价跌了百分之十几，这一脚踢得狠
感觉这个故事在中国的互联网里面经常发生
对，是的，我反正不靠这个赚钱
对，但其实我的问题是这样子，因为我理解这一仗
尤其是现在的 large language model，对任何公司都是不能输的一仗，尤其对 Google 来说
对，现在才刚刚开始，还没有说他输，他只是气势上输了
在口碑，可能是在公众的认知里面，他输了
但是 Google 还有 DeepMind，这个团队也很厉害的
对 deepmind 现在应该有点焦虑
Google 好像把很多业务线都挂到 deepmind 下去了，让他想像 OpenAI 一样来表现，因为它相对也很独立
打包 DeepMind 也太像研究所了
没有一个可能没有一个像 Sam 这样的 CEO 是
对，这是很重要的一个竞争力
OpenAI 很重要竞争力就是他们有CEO，他们是一个产品导向的公司
以及他们人效非常高
团队的人大概是 22 年初的时候， 300 多人，研发，产品，技术都是包括他们的研究员
都是同一套脑子，大家想的都是同一个问题
对，这是一个问题
第二个，他一但产品成功之后，他就有更好的飞轮效应，人才就会往那儿聚
这个是很可怕的
之前被 Elon Musk 弄走的 Andrej Karpathy 叫做什么自动驾驶之父
FSD 之父，应该叫视觉自动驾驶之父，就视觉系的，他又回去了
当然不知道他为什么会走，从 Tesla 可能是被 Elon Musk 给挂起来了
因他觉得没什么用了对吧，就被挂起了，对吧
当时 Elon Musk 把它从 OpenAI 搞走的时候，Musk 就同时退股了，把股份卖了
当时就分家，把他最核心那个人搞走了
现在回去之后，我觉得会对 OpenAI 往 AGI 的道路上有很大的一个贡献
因为他就是做视觉的，现在急缺视觉是多模态
其中一个重要的视觉
是前段时间我看到反正朋友圈里出现了一个招聘广告
类似于我忘了是哪一家公司了，不惜一切代价挖 OpenAI 的华人
我猜回来的应该不会特别的多，在今天环境里面
对这个环境和以前不一样了，这样的国家跟公司是一样的
对，其实 OpenAI 他要招人，他要显示出自己前景很好，对吧
而且他的招人理想是他放了一个很大的目标在前面，我要实现 AGI 通用人工智能
那些人在里面工作，并不是为了钱，我就是为了实现和科学家一样
你想一想，我想是我想去验证是吧
我想实现什么统一理论，大统一理论是吧
他像这样的一种理念在里面，你很难把他人从里面弄走，现在
这也聊到我们刚刚说的另一个成本的问题，人的成本，对吧
我理解刚刚我们说的算力都是在中国的人的，中国工程师的勤奋和智慧下
somehow 听起来都有突破的可能性，对吧
再加中国很好的投资人和 VC，但是人才这个问题实际上我也是比较想跟你讨论的
我不知道这个赛道现在对人才的要求到底是一个什么样的要求
以及你觉得中国有没有这样的人才，如果有，他应该什么画像
我可能不是很专业
虽然我是学 computer science，但对 AI 后面了解并不多了
我感觉从我个人这么多年经验来看，包括最近研究，从去年开始，我从去年七八月份就开始研究 AI，比较晚了
当我们真的看了一些 paper，看那些 paper 上的名字，至少有 1/ 3 到中国人，剩下 1/ 3 是印度人
对，这基本上大概其他的还有什么东欧国家的人名字很多，其实真的白人其实并不多
在这个里面，对你看名字就好了，你可以做个统计也好了
对所有 paper 的，而且 paper 不是中国发的，不中国高校和中国公司发的
你就看微软什么 Google，还有美国大学发的
中国应该占到 1/ 3 以上了
所以可能真正的抢人思路应该是去美国的大厂里面搞 AI 的人
或者是美国的高校里面搞 AI 的中国人，这里去抢人会比较好
其实现在的 AI 的发源地，现在我们叫神经网络的发源地加拿大
加拿大在这一波里面还是只靠 3000 万人口就占到了前列了
对，因为多大的教授 Hinton
他是发明的，应该就是他发明的神经网络，这是他发明的东西
这个应该我觉得不亚于诺贝尔奖级别的
他将来应该是 AGI 之父
不能叫，就神经网络之父
AGI 肯定其他的办法对，但是 AGI 肯定是神经网络，肯定必须神经网络
对
是的，所以在科研来说有这个文化在
一个是在加州的，另外一个就是在北美东部
包括几个大学，包括其实加拿大的滑铁卢，多伦多和麦吉尔
我这么三个大学其实在 AI 方面很靠前竞争
Google Brain 总部分部设置这里的，他们就直接从大学把人拿走
其实中国的公司应该也这么学一下，建分公司的那种，直接从高校拖人
诶，你刚刚你反正你刚刚有提到 Google 在研究上走得比较靠前
就问他们在人储备上是不是做的也还不错
他们应该直接都和，我看的论文一般都是和高校联合发布的
对， Google 来自于Google brain，来自于 Google 的什么 team 来自哪个大学，这样子的一些
他可能就直接把研究和在学校里面成立了实验机构，挂牌带着教授带着学生一块做这种研究
其实中国也是一样的
但是我感觉我补充一下研究方向不一样，这最核心的问题
我之前分享过一个图
要放图了对不对
中国的那个
研究方向真的不一样
北美基本上都是研究语言的， NLP 大语言模型
中国都是研究什么识别的可以快速转化成果的可以卖给政府，卖给什么企业的
我感觉我上次看你那一张图，我的感觉是在美国哈，或者是在北美，关于 AI 的研究更像是对自然科学的研究
对，你说的很对
在中国对 AI 的研究确实比较偏应用科学
对自然
他们确实把 AI 好像研究一个电子大脑一样
是吧
我们怎么去做一个电子大脑出来
这个就是所有的研究都是向 AGI 去的，这个导向是不变的
但是我觉得中国很难往这个方向走，因为你看我们过去这么多年发展来看，我们都是应用应用导向的
不可能说整个国家的思路和公司的思路会变
你想想看，如果你公司有几个研究员天天在这研究自然语言处理
你一点成果转化都没有，干三年就被开除掉了，跟你说
哈哈哈，三年还比较乐观
还比较乐观是吧
但中国其实这样子
我听说达摩院在做得还不错
其实在去年好像是做出来了能够媲美 GPT-3 的一个 large language model 了
我其实你包括我们在讨论大公司哈
其实我个人还蛮看好阿里的，纯个人观点哈
几个原因第一个达摩院还是养了不少人才，人数也不多，也相对比较独立一些
第二个是我自己觉得阿里的卡是最多的，阿里的什么卡
就是他的算力
算力是最多的，百度也很多呀应该
我认为可能阿里会更多，因为他本来的云计算也做得比较好
他有 cloud，对
你看其实像国外的 OpenAI 也好
对吧，包括像 stable diffusion 也好，还是背靠云计算厂家还是蛮多
需要的，不然它成本太高了
对，所以我感觉在中国现在大家在投资 large language model
投我们刚刚说的狭义的大模型的时候，还是蛮看重说能不能够把这些有战略资源的人拉进来的
对的，一般背后都会靠一个云计算公司对吧
大的，微软，有的拉亚马逊，Google 自己都有
对，是，所以反正国内怎么说，我觉得春节前和春节后是两幅景象
我觉得春节前大家是观察学习，当然也很多人被震撼
我相信基本上现在动起来的人都是在去年七八月份，可能和我们差不多的时间
看到了裂变了，开始研究储备和谋划，春节后就全部都按耐不住了
大厂也好，创业公司也好，我感觉是钱和人才都进来了，所以我觉得今年会非常热闹
甚至我觉得在中国关于大模型的投资，上半年或者再激进一点，可能这个季度就会看到一些明显的格局的出现
我问一下，格局是你说在中国不同的厂商之间的竞争就有差异化了是吧
有人领先，有人怎么
我觉得短时间之内产品看不到差异化
还有半年 我指的是哪些团队能拿到大钱
各个大厂将会投多少钱，我觉得在一个月以内，我想可以，可能会见分晓了
所以我们刚才聊的，你可能会比较看好阿里，因为它有cloud，而且他也有专门的研究院在做这个事情，对吧
我也很看好字节
我看好字节的原因比较简单，我觉得好像整个 AIGC 和字节的价值观，组织利益比较像
我个人判断，只是我个人我觉得字节更好一些
对，哈哈哈
为什么我说一下
我说一个观点，因为他有别人可能没有的数据，他有很好的语料数据和视频数据
以及字节是一个屡次成功过的公司，开新业务对吧，尤其是瞄准的主战场，成功过好几次
我觉得也比较容易让人才，让外部的一些资源对他信心更强
而且他们正在做，只是他们没说而已
我现在很多厂都在宣布上一轮是吧，各个公司跳出来说我们在做什么
大模型，其实字节早就在做了
对，可能他只是不需要说，因为毕竟还没有上市，哈哈，没有股价管理的需求
但是我听说内部的很多资源，很多人才的招聘都在为这件事情
对的，而且它是一个国际化产品
对，他有全球的所有内容
对，其实我刚刚 Google 的问题，哈，跟这个问题有点相关
为什么我很好奇 Google 到底能不能，赶超或者超过 OpenAI
因为我觉得 Google 对吧，算是赛道里的棋手的模子，就含着金汤时出身，人才是他的，很多的事情也是
基础贡献也是他的，也是他推动的
对的 但是 OpenAI 不管他是用更好的组织架构，更好的人才，或者是更更加实用的工作方法
在现在这个阶段，哈，我觉得不管是在参数量上，还是在效果上，还是在品牌上，对吧
影响力上全方位的超过Google
所以我首先是很好奇，在美国战场往后，或者是今年明年，事情会怎么发生
变数很多，这才刚开始
Google 不是收购了一家公司吗
投了一家公司，他有一个对标 ChatGPT 的产品
其实这两个产品同时出来的，其实那个更早一些
Claude - C L A U D E
对会不会 ChatGPT 和用户的交互越多，它的数据越好
它会越好
它是 Reinforce 学习了
对对
这个要 咋办呢
这种情况，其实那一家公司也有同等质量的产品，你用一下就好了
可以用叫 POE 产品去试用
几个对比的
对，我回头把链接给发出来
可以 我听说 Google 在号召所有员工在使用他对话产品
是的，对，要的
Google 投了这家公司
其实效果差不多
跟 ChatGPT 可能有一些其他语种上面没有他好，中文或者但是在英文表现上面基本上是同等水平的
是不是有一个可能性，比如当我数据，喂到一定的程度以后，他对效果的提升其实就比较不明显了
不一定，他可以
内部我不确定，因为我不是这方面的研究专家
我没法说数据喂多少会有提升
但是当你数据大到一定级别的时候，它里面会产生内部逻辑
他会产生自我推导逻辑的
对
他自己会 think，就是 CoT 嘛
在你足够大的时候，他会自己学，为什么语言
其实大脑的产生语言不就是神经元连接足够多的时候，大力出奇迹的
大家也相信生物脑这样长，我们为什么硅脑不能这样长呢，是吧
对，当你连接元足够多的时候，他就可能会出奇迹的
他就会有逻辑了，表现出看上去像逻辑
有时候说人类的逻辑是逻辑，为什么机器的逻辑不是逻辑
对大家，因为我们可能意识不到，我们脑子在思考，我们在整理逻辑
那是因为我们有个人意识，我们才知道自己的思考
现在电脑没有意识，他并不知道自己的思考，但是他表现出来看上就像是逻辑
对，没法定义它，所以我们 AGI 也不好定义
你前面说的里面的语料送的足够多的时候，这一点我觉得很大差异
人类不需要很多语料就可以学得很
好
对，是的
所以我建议以后是语料越来越少，我们要衡量一个机器是否聪明，我给的东西越来越少，你就能够把它学出来
而且自我演算，我们叫做什么
叫自己把自己给说服了，或者自我学习了
或者是学会多模态对吧
现在你要教他是苹果，又要教他这个词，又要教他图
图片对，还有闻味道对，对，是的，这样子的对
以后 输入进去 apple 就像一个小孩一样，对吧
他大概知道这个东西怎么念，是什么，什么味道
因为人是多模态的，我们真在三维空间里面，所以机器现在做不到
但是我觉得很快就做到了
现在有 DeepMind 就有平台在做
我在上一个课里面讲了，DeepMind，他就做了一套智能代理
多模态的，而且可以感知世界的，还有大语言模型的
对，他往下一步走
我觉得现在很多公司，国内的公司是大家都看到，现在我觉得好像又落后了，人家其实已经在那了
因为我们现在要先解决有这个原子弹的问题
对我们来说，我觉得今年对吧，如果我们能把原子弹造出来，已经是巨大的胜利了
其实造的出来并不难，只是效果好不好的问题
对，肯定也要效果上也不能太差
我刚刚讨论了很多美国的大公司在做什么哈，因为你长期生活在北美，所以我也带国内的同学们提个问哈
现在美国 AIGC 的创业公司们主要在做什么
这个就很多了感觉，AIGC 我觉得不能够只用 AIGC 来定义他们做的事情
因为举几个例子，我们可以分类
一般现在会把它分成几个层次
大模型可能最下面是算力层，这个是核心的
硬件层的算力层会有什么
模型层 模型层上面就有 middle layer 中间层，中间层上面应用层
其实在模型层，在算力和模型层侧面还有一些叫做基础架构层，基础架构 Infra
每一个层上都有很多公司在做，对每个层上面，在中间层和应用层是最多的
因为按照无线手机应用的发展来看，也是这样子的，大家做手机操作系统的少
现在可以很明显，你可以把 OpenAI 或者未来它的产品 API 当成水电的调用，它就是水电
它就跟操作系统一样，或者它比操作系统还要再底层一点
它就是一种能量，或者是一种智慧，一种带着这种叫智力的智能流
这样的 intelligent 的这种 streaming 的东西，你反正你喂他什么东西，他就告诉你什么东西
他就像这样一个机器一样东西，叫智能机器，它雏形，智能机器的雏形
所以我们不好用操作系统来定义它，但是它是管道，它是 Pipe，它是连接上面所有东西的这些管道
北美的公司，我觉得很多公司要做两件事情，第一个是用它把之前的东西换成它
就是 Notion
对，把底层换成它，对，有很多现有的这种公司在变的智能化
说的很对，包括 Microsoft office
对是
对不对
其实 Google 的那套 Workshop，它叫 Workspace 那一套，现在改名字叫 Workspace 那一套
也在变智能化，他们也在用他们自己的技术，但是它不是用的 Openai 的
对，用他们自己的技术，让他也能够向 Microsoft 微软展示的那样
其实在去年的时候，我分享过一个视频， Google 就展示了快速的怎么去做 PPT，我说话就好了，你帮我自动做
但是北美有很多生产力效率这一级，做的最多的就是生产力工具的提升
除了原有的这些公司，比如 Adobe 是吧，他们都会做的
所以我感觉这一层，旧有的这些非智能化的工具，它转向智能化带来的威力会很大的
很多小公司想要赛道里去抢一杯羹出来还是挺难的，除非是你的工具特别 sharp
这个里面其实我跟很多国内的朋友也讨论过，哈，里面就分成这些老工具，可以把自己智能化
还有一些可能它的 workflow 就彻底变了
比如我们以 Photoshop 为例哈
photoshop 它以前是OK，我有一个图片，我来思考这个图片是图层的逻辑
我在每一个 layer 上去加去改，但我觉得好像在 AIGC 的世界里面，它不一定是工作流了，可都不一定是个逻辑了
对，这是个问题，所以会有很多新工具出现，它会形成一个分叉
有的工具是对话式的，我告诉你，我说什么，你画什么
我说你画对吧，对就好了
Photoshop 里还是我画
这个地方我说一下，我再补充一下
对，再来画
现在就有插件的，我用过一个
对我用这挺好玩的
对我打个比方
哈，这个比方有点像什么，现在哈
前 AI 时代的企业都可以算作是传统企业，他们的 AI 化的难度有的时候可能跟传统企业互联网化的难度会有点像
没有那么大差别
没有那么大差距，但他有可能要，如果一旦涉及到，他要彻底的改掉自己的工作流就比较难
我跟你意见不一样，传统企业是完全非数字化的，他要改，他要他是劳动力的成本的
他需要改变完全，他要完全转变，他要完全转变成数字化的是比较难的
但是现在的这些传统的工具，它就是数字化的
而且它也是多人协作式的，它想把智能化东西集成进去会比较容易的
而且我觉得他们愿意集成的，他愿意集成，他要为了守住自己的市场地位
为了让他的用户别流失，他要保命，他必须要集成
对，这个肯定是
其实我在想的是一种可能性，哈，有一种可能，在 AI 以后，其实工作流的起点变了
对更多的是一种
怎么说
就是 a i g c 或者 a i c g，反正怎么说都可以
我在第2讲里面，第二堂课里面分享过
最后有一个流程的变化
其实我们更多的变成了一种助手
软件变成助手了，你并不是说你要雇一个助手去用这个软件来帮你做，你现在直接跟这个软件说，他就帮你做了
是的，这样的逻辑
对的，所以就变成了你的整个创业的做的工具对吧
你要思考的事情是，不是说OK，你是不是替掉一个人
对吧，而是你想
如果我作为一个很好的 AI 工具，我应该怎么去嵌入工作流
所以我自己感觉可能有很多在做应用层的 AI 的公司
最大的一个难点是怎么在 AI 的新的环境下去重新去思考他服务的对象怎么工作
对，我个人玩了一下
我现在我玩生成图片玩的比较多，玩的比较深入了
对画图，我感觉现在比 ChatGPU 好玩
但 ChatGPT 我用它就好了，因为没什么难度
我也不用配置，我也不用get，我也不用训练
太简单的用起来，你只用问他问题好了
但是用 stable diffusion，它是开源的，你就可以用很多 web 上插件把不同的模型合并
训练出一些很奇特的画法出来，这个就有很组合性
其实现在有很多画师也在干这个事情，但是会让画师发稍微比较吃力
因为你得会一点代码技巧，你会做得更好一些，程序员会做得更好一些
所以现在过程还非常粗糙，就跟什么一样，跟操作系统刚刚刚诞生一样
我还得把很多库我自己组合，我自己来用，它根本就没有用户界面，什么都没有
现在处于这个阶段，我觉得这个里面就是金矿
对，所以创业公司要做的是用户界面
对，其实就是要把 Prompt，因为 Prompt 是万能的
其实 ChatGPT 它不是推出 API 之后，它做了一个什么叫做什么 ChatML 是吧，一种描述语言
我觉得这个里面有很大机会
其实我在这个里面我还有个困惑，因为我觉得大模型的发展也还在发展当中，对吧
可能 GPT-3
5、GPT-4、4.
5 和 5 都会给我们带来非常不一样的体验和 feature
底层大模型在快速地发展
刚刚我们说的科研也在快速地发展
应用层，它永远都是基于当下已经成熟的技术
如果咱们最底层的操作系统，它进化得特别快，对这个时候应用层咋办
提心吊胆
哈哈哈
对，这样你现在说诶，我用 ChatGPT 做一个让他可以认识图片的功能是吧
比如我，现在我可以用 CLIP 把图片转码了，变成一张文字输给他，他就可以看懂图片的
但是现在是你人工做的，你做了一个组合界面，哪天 GPT-3 它多模态自动就把图片给消化了
你不白做了吗，这功能
所以我感觉我自己感觉可能哈
创业公司确实第一是学习的压力变得很大
随时随地要关注新技术，同时一定要把垂类的应用场景
对，我感觉是你说的很对
最终用户还是需要通过一些界面来使用东西的，是吧
他要记住一个品牌我要用什么东西，ChatGPT 不是万能的
但是你需要在创作工具或一些特别的工具工作流上面把用户给绑死
即使智能，智能的底层再怎么升级，用户还是用你的好了，因为你只用换引擎就好了
对的
所以工作了要灵活
比如我想象的是我做的工作流，是服务于三五年后 AI 极度成熟的工作流
等技术不停地革新的时候
OK，所有的武器都能为我所用
可能是这样的一种状，但我觉得要求很高，对吧
要有，除了要有产品能力，要有技术能力，还是要有一点点前瞻性的眼光，要敢于预判
或者你直接做更靠前的事情也行，对吧
在一些多智能的发展方向，你现在都大语言模型
你可以再往前看看视频这种图像混合声音处理的，把它都集成起来的方案，对，这个也是可以的
但是大语言模型下面另外一个方向，就是你可以带到业务里面去，是别人做不到的
比如某一些场景，培训、教育，英语学习
是的，你之前在做这个事情，你还是在现在在做这个事情，而且你会做得很深入
你知道你的客户，因为 ChatGPT 不知道你的客户，对吧
你可以把客户留到手上就好了，正好只是换引擎
这个事情我还听到过
好像有国内有朋友在拿 ChatGPT 做面试的初筛
正好让 HR 在面试之前先让 candidate 跟 ChatGPT 去聊一会
但我很怀疑他有没有节约人力，因为 apparently HR，对吧
还要去把每一个人的答案读一遍，读完了以后还要去有面试
所以我不知道在今天的结合深度能不能真正地做到提升效率，但我相信未来一定可以
应该提升了的 北美疫情以来，好多公司都用 AI 面试的，我忘了产品的名字了
但是我听到报道，看新闻，大家很多工大量采用了
特别是对一些不对像程序员，不是程序员这种工作，或者一些比较职业性，流程化的工作
他问几个问题就好了，都不需要人问的
这种他需要新的语料库吗
或者新的
他公司就有 knowledge base 的，而且之前也不用 ChatGPT 这种牛逼的产品，就是一些条件选择就好了
你去做一些题，其实就在线面试，可能会有一些 AI 分析一些答案，这种东西很成熟了，大量采用了
我们刚其实聊到了很多 AI 对创业公司，对大公司的影响，对创业公司的影响
对我们现在可能落到个人一点
请教一下 Indigo 哈，你怎么看 AI 对将来这个人的影响
我觉得影响从现在来看，我觉得是巨大的
我觉得未来人会分成两种人，一种会用AI，不会用 AI 的一个是不会用 AI 的，对他没法和 AI 和谐相处的
有一种拥抱 AI 包的很好的
就跟之前手机时代，有人会用手机应用，有人不会用的是吧，老年人就用不了，对不对
再往前面走，有人会用互联网的，人不会用互联网的
如果再往前面走了，有人会写字，有人不会写字的，永远都是这样的一个区分
而且 AI 这也是带来区分，它是提高的个人的生产力，这个是非常可怕的
个人的生产力以及个人的知识的运用能力
我看到很多人说你让 AI 帮你去看书，帮你摘要出来，我其实也其实这个有点夸张了
我总在思考有一种方法，因为我们都在学习，我觉得 AI 对教育改进会非常可怕和恐怖的
我最近写了一条Twitter，我也写了这些
这个是我一直在考虑的问题，因为他会怎么去颠覆教育
因为我们所有都有小孩，他会怎么颠
我现在让我小孩尽快的了解，让他去用，我不阻止他
我还让他做了一期 podcast，和 ChatGPT，你得适应他，而且还有一个你会和他，就刚刚说的
你说第一种，第一种方式就是进入工作流
如果你总是和 AI 格格不入，用不好，它就没法进入到你的工作流，对你就没法提高效率
当然这个东西是双向的，有很多的产品，创业公司需要把整个变得更简单，让用户更容易使用它，这个是肯定好事
所以这是创业方向
把 AI 进入工作流变得更简单
就可以获取，用新的用途获取更多用户，这是第一个方向，这是很好的一个方向
第二个，我觉得 AI 对于教育和对于知识学习，因为学习是长期的，不仅仅在学校的时候学习
有了 AI 这样的，在我们有的搜索引擎之前，你可能忘记了
你在有搜索引擎之前你是怎么找资料的，你记得吗
不好意思
搜索引擎之前我还是个小朋友
对，所以说，我们现在的小朋友下一代
诶，有 AI 之前我是怎么学习的
我都忘了
对，是的了
就会有这样的结果
所以我们怎么拥抱 AI 学习，怎么建立知识自己的知识结构
因为你把一篇一本书让 AI 帮你看完，那本书还不是你的
而且你每次要调用的时候，你还去搜索，所以大脑会我觉得会有个好处，让大脑更节省能量了
让人类的智慧用在哪儿呢
更加抽象和更加结构化，更加帮知识编索引就好了
我不需要去记很多东西
对，事实上，在照相机出现之前，人是要自己记住图像的嘛
要记得，那时候记忆得很好的
是的
所以现在我伴随着科技的发展，人类需要记的事情越来越少，纯靠记事情越来越少
所以我们应该锻炼，把这一部分记忆的空间让出来，让给逻辑大脑再成长
有可能我觉得有赖于老脑学的进步，我们并不知道记忆空间现，大家不知道记忆的原理
海马体是有记忆，但它是短时记忆
有很多记忆不知道存在哪的，也不知道怎么产生作用的
所以我感觉我们会把一部分的硬记的东西给放，释放出来，变成训练逻辑化
这种逻辑化就是如何让一个知识在脑子里快速进入，建立结构
这个挺重要的 其实学校没教过你的，学校都是在教你怎么记是吧
考试
但是把知识结构化这个是特别重要的，而且任何一个知识都可以结构化，而且结构化你可以怎么说
不用自己来做， AI 可以帮你结构化，可以分享这些结构，你只需要把结构记下来就好
我们现在讲课一样，我把我的课程输入给你的时候，我告诉你的是什么
我告诉你并不是 AI 现在 12345 它在做什么
我告诉你脉络是什么
你再回想一下我们讲的课，你又很清晰的从历史什么，过去是什么，现在是什么
你有一个结构在脑子里面，我给我映射进去了给你，map 到你大脑里面去了
其实以后 AI 可以来写这种教材，或者或者人来做这个事情，让 AI 辅助你把它快速地 map 到你大脑里面去就好了
你也记住了，你也把这本书看完了，剩下的具体的细节调用你检索就好了
你问他就好了
或者以后装个什么脑机接口，你都不用问了，你想象就好了
不过有脑机以后，我觉得 AI 可能起到作用，这也可能会更不一样
因为现在脑机来读你所想已经读出来了
不是前天有一篇文章说把你大脑里面想的图像给用 stable diffusion 给还原的吗
对吧，做得到的脑电波进一步的读取你
我在想什么
我动作是什么
你装个脑机，戴个头盔就好了
以后可能都不用戴头盔，戴个眼镜，什么连个线在后面对就好了
我在想什么
马上他就告诉我了
可能以后人类哈
在脑机出现之前，最重要的能力是 Prompt 能力，因为 Prompt 就是你跟 AI 交互的能力
对，而且 prompt 我觉得所有的创业者其实都在做 Prompt 这个事情未来
prompt 所有的创业公司都要做 prompt，因为 prompt 会很复杂，以后会
而且不是 ChatGPT，它已经出了 ChatML 是一种描述化的，结构化的描述 prompt 的语言吗
我总感觉现在我们这样子，因为我现在去写一段话
用人类的方式他自然语言模型去让 ChatGPT 回答你一些东西是可以的
但是你写一大段话让他生成一个图像，是的，很难，以后很不标准
所以现在在 stable diffusion 里面就有很多工具
什么 controlnet 控制你的动作，给你画一个，把一张图上面这什么个区域画什么，怎么重叠
你还是要用辅助的图像的方式去教他
这个东西也会变成一种新的 layer 的语言，或者一种界面
你现在还只是图像这么简单的东西，以后 3D 怎么办
声音怎么办
都合起来怎么办
是不是有更好的交互工具来做创作了
是，这都还没有，现在都是空空空白
都
还是可以跟你聊一聊
觉得可做的事情实在
太多了
新大陆这是
对诶，都不用担心，没事做
其实我们已经聊了一个多小时了
对，我们可以用几个问题来收收尾
我们俩彼此提问
好吧，我有个问题想要先问问
今天 感觉是浩明在主持的，我是嘉宾的
对，我好奇一个问题
对，你觉得现在 AI 哪些地方是做得很不好的
有很大的提升空间的
很不好了是吧
我觉得看到什么方式应用了
因为自然语言不是像大语言模型出来的撰写什么东西
短的逻辑撰写表述很好的，但是强逻辑表达还是不行
包括强逻辑表达真的是适合我们思考结构的这种表达还没有，他还只是过去的语料分析呈现的
刚刚第一个问的 notion 问题，其实为什么它降价有优势，它把所有市面上关于降价有优势获得一个排序
对是吧 讲了一个比较好的车轱辘话
很好 对，但是这个对于 common 够用的
但是你真的对于自己去思考这个问题的时候，他的代理思考，其实我觉得这是一种危险，他会被带偏的
他是基于自己的语料训练出来的
他并没有有一个像人类这样的，这种我没有太多的语料，我叫构思，这样的一种能力还没有的
我觉得这个是要改进的地方，而且如果不改进会很危险的
如果大家都会觉得它会被极大的污染，它会极其依赖于语料库的训练，你会污染它
如果它真的是有的像人类一样独立思考的逻辑，它就不怕了
不怕这个问题了
他改进了也很危险，改进了很危险，更危险的
科幻小说里面一切的可以作恶的 AI 就诞生了
对，所以为什么现在 AI 是没有动机的
我觉得现在人类做事情是有动机的，有目标感的 AI 是没有的
但是我觉得目标感得等我们把目标感怎么形成的研究清楚了， AI 就会有了，我们教他就好了
很可怕 对，其实更可怕
第二个大语言模型，我们可能会围绕大语言模型之上做叠加的工具还很早期，现在还
所以很多工作流其实它效率更低
比如我想用它来画图，画好，其实还不如我自己画张图
有时候是这样子的，所以说效率的 gap 还在很大，需要很多人去参与开发来优化它
其实我觉得为什么我在观察，一直在观察
stable diffusion 有很多插件
它真的就是一种野蛮生长的方式，因为它完全开源，大家可以跟它做任何东西
你就可以看到他用最快的速度在进化
他不是说一个大公司的产品放在那儿它不开源，程序员的背后玩
我只能每天，你今天发一个版本，明天发一个 checkpoint，我才能用
现在不是的，现在都是插件都领先 checkpoint，很多融合都到它前面去了
这种现象就像生物的进化一样，我看到很好玩，所以你要进入到社区里面去看
基本上是程序员和设计师在做
主要程序员，主要程序员在这里面我就会，我最后会进去观察他
这事情很有趣，我感觉现在他很弱
但是你会发现在这样的一个环境里面，它爆发出来力量会比早期的移动应用还大
因为它是智能融合，它就是每融合一次就会有奇迹诞生，因为我在我的第一讲到了软件 2.
0
软件 2.
0 的概念是 Andrej 提出来的 Elon Musk 招过去又走掉的那哥们，现在回 OpenAI 了 FSD 之父
他总结几个优点：软件 2
0 代码不是你写的，是训练出来的
神经网络是长在一块的，你想提高它的效率，你把两个网络连起来就好了
合并合体，它一合体，它就能干更多事情了
而且你减掉一块也没关系，只是精准度下降
砍掉一块就跟大脑一样，剁掉一块，大脑也可以工作，但是我们没法把两个脑粘在一块去
那是靠人类语言连起来的，人类能够协同就是靠语言，我们语言就是一种智能的压缩
其实我未来觉得人工智能不就是这样的吗
你把它很多连起来，通过网络，但是现在你可以让它长一块去，人脑没法实现
但是电子大脑是可以实现的，让它长一块去，你就可以把很多模型训练在一块，它就会越来越厉害
可能硅基生物是一大坨，我们碳基生物是一个一个的
对的，我们碳基生物没法就长在一块去是吧
对，长不到
一块去，得把真正的磷肉结合
但是硅基可以的
对，硅基可以 只要有足够多的能量和算力就好了
对，差不多这两个
我问一下你的好
对，因为你在，我们聊了很久了
对，你一直在看国内市场，因为这几年我都不在国内
对，从你去年开始，因为去年我们年终就开始关注 AI 了，一直到现在
你感觉在中国市场上面，你会投资什么样的 AI 团队，或者你觉得什么样 AI 团队会更好来改变现在的这些业务
对，这个问题比我问你的问题要小很多，但我先勉强回答一下
从我个人的角度
我待会再问个大的
对，首先是我认为在今天的中国投大语言模型是一个must，这个事情我们对我们必须要投
因为原因是，首先这个东西是很价值观的
语言模型有自己价值观
很价值观的
我觉得为了保存我们对中文的文化，或者中国人的这种思考，或者我们自己肯定是很有必要的
不管是从国情，从我们保存我们的人才
这个事情对文明是很重要的事情
所以我觉得这个是一个must，一定要投
我觉得很多人都看到了，但是这个东西它不是一个便宜的游戏，我觉得他的入场券就很贵
对，所以我是觉得需要有更多的厉害的，像 Sam 一样的人出来带头来 lead 这个事
第二个是我非常看好的，也是我觉得创业者和我这种年轻人
或者是偏年轻一点的 FA 跟投资人很大的机会是应用层
我觉得中国人的产品能力，从互联网时代到移动互联网时代，真的是在世界范围内都有证明了自己
我觉得我们一定能在产品上做出来，我刚刚说的更好地融入工作流
我们在产品上的 vision 能够让我们做出来 AI 时代的最好的产品
对，这个是我相信的
所以肯定要做应用层的
而且很多工程师这一次更容易国际化了，对吧
因为你做你可以直接挂 GPT 好了，国际化的
是的 第三个机会，第三个是我觉得我个人的一个畅想，哈
我觉得我还是希望你能不能有一些特别优秀的中国的创业者去想想下一步
我感觉首先第一哈，我们现在打的是个追赶战，你做得再好，你也是第二名，哈哈
你不可能在，尤其是在 AI 对吧
靠数据，靠钱，马太效应或者规模优势还挺明显的，网络效应很明显
所以我在想还是期待我们中国这件事情能启发我们很多中国的创业者，哈，看能不能走到再下一步
比如我觉得我们在移动互联网，是在美国打开了移动互联网的大门以后
我们做出了很多再下一代的产品的，我非常骄傲
字节这样的公司对吧，我就生活在同一时代，太牛逼了，太厉害了
对
OK 了解
第一个是 must 大模型，第二个是应用层对吧
这个我们观念是一样的，对我的第一个有不一样的观点
对我的第一个 must 有不一样的观点
我再问你第二个问题，你刚才说问题不够大，格局
对我个人，实际上我之前也发表过这样一些言论
我经常在朋友圈或者 Twitter 上说，我觉得 AI 这一次进化
AI 这样的技术，包括我们前面去年很火的什么 Crypto 这些技术，或者 Metaverse 这样的技术
它本身就是无国界的，我觉得它应该是代表
因为每次人类政治结构的进化，或者社会架构的进化都是技术进化带动的，技术变了，它就变了
对工业时代，什么农业时代、信息时代，我们现在这一次进入了智能时代
这些技术就不应该，做的人不应该只看到我的国家或者我的民族怎么样，对他全人类的东西，他就全人类
在下面你怎么来看
观念如何来看AI，它代表全人类的这样的一个方向
大家是不是应该以自己的民族优先来做这个东西，还是以人类的角度优先来做这个东西
我非常喜欢这个问题
首先我立场鲜明地表达我的观点
我觉得首先智能时代是 cure，它是解药
因为在今天这个时代，我觉得大国之间的矛盾、文明之间的矛盾已经很难彻底地被调和了
他会有缓和，原因是什么
原因我觉得你看在就过去，对吧
我们有很多东西又到了渗透率100%，包括我们的生产力，也可能很难再提上去了
每一次新的技术革命都能带来社会的生产力的巨大的提升，它可以重新让我们对吧回到一快车道
它一定是我们现代很多矛盾和冲突的解药
其实我觉得我们都对我们长大成长的世界太习以为常了
其实我们仔细想想哈，我们现在的世界格局，石油也诞生没多少年对吧
我们现在算是一个内燃机时代，不管是在新能源，在技术上都有很多的改变
我觉得有可能哈，因为我对 AI 这个事情确实觉得是打开了一个新篇章
有可能在漫长的历史里面来看，移动互联网只是新篇章的序言
对，所以我觉得才刚刚开始这个事情
就是因为随着生物技术的发展，我们可能会活很久
我们可能都能活到 150 岁，现在我相对于我们 150 岁来说都很年轻
年青年儿童
对，所以我们活得久一点，对当下的得失看得淡一点，有什么机会冲进去干
这是我觉得对这个事情我的一个看法
我是觉得这当下的矛盾冲突对吧，都是是比我们年长的大人物去解决的
了解
你好像绕开我这个问题了
没有 我觉得我观点很鲜明，他是个 cure，他是解药
一定是人类优先，而不是民族优先的
对，我也是这样考虑的
对，所以会造成很多思维方式有点不一样
anyway，所以我感觉 AI 的技术应该是全人类共有的，不管是哪个国家做的，都应该是共享的
觉得倒是你刚刚说的话题给了我一个启发，因为我不是技术出身，对吧
我觉得现在可能还不管是创业者也好，或者在里面真正的 builder 也好，都还是需要技术能力的
也许将来某一天对吧
等我再发展，我去做个什么 AI 社会学家
以后肯定是一个很热门的学科
对，什么
我准备在第四讲里面 讲讲这个话题，因为第四讲会讲脑科学一部分
会讲一下 AGI 的方向，以及 AGI 诞生之后世界是什么样子的，这个是挺好玩的一个话题
而且用什么方式能够来达到 AGI，这个过程其实也是比较有趣的
今天我们聊了快 80 分钟，将接近 80 分钟
首先很感谢李厚明花这么多时间，大半夜，我这儿早上跟我聊这个话题
当时李厚明给我这一些问题，我觉得挺有趣的
我感觉这一期会是一个知识含量度比较高的期话题
也希望我们这一期对谈能够比较浅显的把这些东西给传递给大家
我们不要聊太多复杂的问题，我们可能只是聊一些 AI 的发展应用和现在的面临一些创业机会的问题
所以很感谢李厚明参加，看厚明有什么
谢谢 Indigo 我是强烈认真的给大家推荐一下 Indigo 的课哈
因为这个课对我的启发很大
你刚刚讲到的学习方法的问题，我觉得你的课里面把 AI 发展的历史梳理了一遍，对我的帮助是很大的
我觉得大家可以通过看 Indigo 的文章，看他的视频，跟他一起找到一些在 AI 时代的学习方法
我补充一个，我自己想做一个产品，要把我自己的学习方法，阅读的内容
用 AI 做一个 AI 的助手，来帮我来解决这个问题，更高效
所以应该叫做一个 personal 的 knowledge base 的这样的一个代理
智能代理
所以我也不能叫创业，我就做个实验，如果好，我们就可以把它变成一个可以给大家使用的产品
可以对，我把我自己的经验全部集成进去，应该会是一个什么样的东西
回头拭目以待
好的 personalized AI 以后你就是线上生活的 her him
对，好，非常期待
谢谢
好，谢谢大家，这期结束，拜拜
