欢迎回到 Indigo Talk
这一期我还是邀请了我们的老朋友李厚明同学
他今天继续来帮我们主持 Indigo Talk
后续我们可能会这样长期配合
我邀请了一个女性主持人
会让这个节目很有趣一点会有不同的视角
然后另外今天我们邀请的一位嘉宾是来自Google的 Qilin
Qilin 你自己来介绍一下吧
大家好我叫 Qilin
我是现在在Google做TPU
是TPU的设计的engineer
然后我今年是刚从美国杜克大学博士毕业
我的这个博士的方向基本上就是人工智能芯片设计
然后我这个方向来做了从
我的本科在北京大学读的
读本科的时候就一直在研究
从2017年2016年那个时候开始
开始做人工智能芯片一直做到现在
然后现在我就是到了这个公司里
Industry去真正的去做这个产品级的东西
然后很荣幸跟大家分享一下
我这些年对AI芯片这个领域的专业知识
然后希望能给大家带来一些这个insight
对我很需要
因为我们的节目好像这个DeepTech越来越深了
对这个已经到这个TPU和GPU了
对因为之前可能讲的还比较泛一点
我觉得我们后续会陆续的邀请各个行业的
或者公司里面的这个有独特视角
就是一线的工作的同学们
来给大家分享现在的AI科技或者是深科技
或者说是一些社会上有些比较有趣的话题
所以说我们今天就很荣很荣幸的邀请到了 Qilin
因为他做 TPU
厚明你可以来问你的问题了
好的 我先说一下我代表的视角
我主要代表的视角就是普通人视角
因为我总觉得就是Indigo怎么说都是
这个创业者加上技术出身
我就是干投资干了很多年
基本上技术也不是很懂
勉强的看过一些商业模式
所以其实我在准备这个问题的时候
是 Qilin 和Indigo都帮了我很多很多的忙
而且我越准备我觉得越有意思
因为今天的整个GPU算是AI发展的一个基础
对吧,就是很多这种 Machine learning啊,
包括现在的一些AI的框架,如果不在GPU上也成长不出来
所以呢,对,我其实是非常非常想从这个历史先了解起啊
就是 Qilin 你能不能这个非常简单的就带我们过一下
如果一个普通人想要了解到这个行业,
应该先知道哪些知识,过去十年发生了什么
嗯,好的,好的
我就可以先从基础的计算机架构知识可能开始简单的讲
可以快一点不然我们的时间不够的
对我们简单不会讲的太多
快点那个PDF
对我们放一下我做的这个slice
简单的讲一下我们就是随便聊聊
先从这个基本的计算机架构开始讲
我希望可以给大家讲的就是清楚一些
首先我们是说这个计算计算到底在干什么
我们这 hardware computer到底在做什么
我们就想简单一点
我们把它想上一个厨房
我们在做饭
对吧
然后你这个计算机架构
设计就是说厨房怎么设计
那么我们整个系统里面有core
有计算的核心
那就是我们厨房的台面
然后我们也有存储器memory
它就是厨房里面的冰箱
然后我们做饭
我们需要有算法就是菜谱
一步一步怎么做
这就是有这三大核心要素
我们就可以去把这个做饭这件事情完成
然后storage
就比如说我们为了方便放一些东西
我们还可以加一些storage
这个就是相当于是cabinet
就是那个上面挂的那个顶柜
所以整个这一套就是一个最基本的一个计算机
也就是说cpu
整个 Silicon valley 硅谷
就是基于这样一个最简的东西开始往前走
然后那我们肯定是这是80年代70年代的东西
我们怎么样到90年代00年代我们在干什么
就摩尔定律在干什么
摩尔定律所提供的就是说我们可以在同样大小的
这个 silicon chip 上放更多的东西
因为我们的那个
比方说以前你是1微米现在变成
变成那个 .18 了
你就相当于是面利用率可以到4倍
然后现在可以放更多的东西
放更多东西就是相当于你在厨房里可以加更多的东西
让它更快
类比到我们厨房的这样一个优化
也就是说
之前你做饭切好东西
你得往冰箱里放
然后你每切一个你往冰箱里放
然后再拿出来再切
就很麻烦
所以说你在中间引入 Cache和 SRAM 这些东西
就是存储器
相当于我们就是引入一些plates和盘子
我们加这盘子以后
我们就可以切好东西
随手先一放
之后要用的时候再从很快的去拿
然后那个指令集,指令集就是说我们的这个灶台可以完成的事情
我们让它越来越复杂,就是CISC, complex的一个 instruction set
然后也就是说以前那个锅可能只能煮
现在我又能煎又能炒又能炸什么都能干
这样的话它那个,它代价就是说它的资源会更复杂
但是它好处就是它会执行的更高效
然后以及有其他的一些优化
比如说并行,并行就很关键
就是说我们有两个灶可以同时做两份菜
这个是很重要的一个优化
然后呢CPU的核一样的核
对我们多了这个比例很好
对就类似的那个东西
然后DRAM就是说存储器
我们知道Memory
我们相当于是CPU主存越来越大
那主存就是说我们那个冰箱越来越大
你可以把更多的临时用的数据放到冰箱里
而不是说你跑老远去仓库里面拿
然后包括 Hard Disk
也就是说我们有更大的这些东西
这样我们就让整个厨房变得越来越专业
越来越做菜出菜越来越快
这都是硬件上的
当然我们还有软件上的
就是说菜谱做的更好
对吧
我们菜谱做的更高效
就是所谓算法优化
就是什么时间复杂度
空间复杂度优化
就是说我们这个菜做的更高效
这就是CPU的优化
这一步一步
然后我们再进行极致的再往上走
就是90年代到2000年
有更好的,更好的CPU,继续往前做,但它还是厨房,还是那个私房
就是说从家庭厨房变成了那种开私房菜馆的厨房,但
它本质上还是一个,可能是以前是家庭主妇做的
现在是国宴大师做的,但它出菜还是一道一道出
它可能做的菜越来越复杂,但它出的还是一道一道
比如说它用的工具会变得越来越好
它的那个原来会有一个SOC嘛,就是系统
很多东西都放到一个厨房里了
然后 L1 L2 L3 的 Cache
也就是说这memory更多
那个碟子把它分类分了更多
有大碟子有小碟子
小碟子放就临时放的就是传菜用的快
然后大碟子就是用来用来这个
用就是相当于是他去考虑那个食材的
操作就是操作的那个次数
和他被被移动的次数
让他尽量去决定他放在小的还是大的里面
然后就引入新的新的 memory HBM
就高带宽的Memory
然后以及这个我固态硬盘
就 Solid State
Disk SSD
也就是说他那个冰箱呢
他那个拿东西更方便
所以我这方面就是CPU上的优化
就 Intel 时代的优化
Intel 从这个这个这个
1微米做到这三纳米
7纳米
14纳米就是一步一步的
基于这些东西往前走
麒麟
我提个问题
这些比方是你想出来的吗
还是这是一个这个
行业里面通用的比方
我我我自己琢磨了之前琢磨过怎么样
我觉得我觉得这个太好意思了
首先是这个这个非常拉近和我们观众的距离啊
就是即使我是一个这个既不做饭
也不懂技术的人
我觉得都非常的清楚
然后这个图片我记得你说是我们一整理做的
对吧啊对
就图片全都是 Ai 生成的
就是你这个图片太懂你了
要他要去生成什么样scale的这样一个
这样一个对
对,这个风格,对,这个风格也非常的 geek,对
难怪这个说老黄的这个爱好是这个这个这个烹饪,烘焙
可能,可能有那么一丝相似之处,这很有意思,很有意思,这本质上这个
厨房的优化和计算机的优化是一样的,就是说他在
他就要完成一个特定的任务,然后在那个任务中让他更快,
嗯,但是CPU,好,我们说一句CPU,就是因为他他
是给那个desktop,就是个人电脑处理的嘛
他处理的任务是非常 worsetile 的
也就是说他什么任务都可能做
他可能用来打游戏
可能用来处理文字可能来怎么样
他必须要满足什么都能干这样一件事情
所以他需要像这种
就是说他不断
要去处理更复杂的菜谱
所以他需要去像
像你从一个家庭主妇的厨房
像国宴大师的厨房去优化
但是GPU就是另一个故事
GPU是什么样子的
GPU你可以把它想成餐馆
就GPU在最开始的时候是那个G是什么意思
Graphics是用来处理这个图像
图像的 对
说我有一千个点
我有一千个点
一千个点
那么我这一千个点可能要干类似的事情
给一千个点算一个颜色
然后把它把它呈现出去
我有一千乘一千个点
这样现在越来越高清了嘛
你会发现它每个点干的事情是相似的
也就是说呢
你需要更多的厨子
你需要更多的人去干这个事情
而不是说一个
然后你现在在serve一个餐馆的大师
一个人餐馆
然后你一个人点了婚宴
16桌都一样的东西
对吧
你现在还让一个国宴大师
给他一道一道的烧
这就烧得很慢
然后GPU的好处就是说
我是个餐馆
我原来是个complex的core
至于CPU来说
我现在没有那么complex
把它做简单一点
你每一个学徒就可以了
你不需要是大师大概能干明白就行了
然后你像那个CPU是multiinstruction
multistruct就是说我好几个核 可以干不同的东西
现在不需要了你不同的核
你有一千个核你干一个事情
干一个事情就可以
然后它的 global memory 然后变成GPU的memory
就是它自己内部的memory
就是把它作为一个协处理器
和放在这个主的CPU旁边
就是这样一系列优化就会导致它什么
它可以提供巨大的并行度
同时它又可以做 general purpose的处理
这个是叫GPGPU
就是 General purpose GPU
在这些年
通用目的是吧
通用目的就是GPU
就说GPU的发展的时候
一开始它就是做graphic的
然后在2006年的时候
就是NVIDIA引入Cuda
打算用GPU去做通用计算
就专门处理这种restaurant
这种类似的这种计算
所以一个人的炉灶在前面每个人一个
对每个人一个
每个人一个
但是这个炉灶的关键点就是他还是什么都能做
可以煎 可以炸 可以炒
和CPU一样只不过做的没有那么好
所以它是 general purpose processing
叫GPGPU
然后GPGPU最早应用
最早比较有意思
因为就是挖矿
挖矿
就是一开始大家发现CPU算的很慢
但是挖矿这个东西
它那个算法很适合并行
大家用GPU开始挖
对挖比特币
然后我记得是10年左右
英伟达那个时候就已经快不行了
游戏市场不大行
然后靠GPGPU挖矿就又起来一次
就是类似的这个东西
这是GPU
然后GPU我们就重要的特点
它可以提供巨大的并行度
就是算那种归整的
归整的任务就算得很快
然后第二个是它通用 General purpose
然后我们就之后呢
就是说这个TPU或NPU
这是你们专业的特别解释一下什么是TPU 我觉得很多人不懂的
TPU 的t指的是 tensor processing unit
tensor 张量 就举证
tensor processing unit
然后NPU 的n指的是neural processing unit
neural是specific for neural network
就是说神经网络这件事情
这很有意思这有意思在哪里
就是2016年那个时候从a到z
apu bpu cpu全都开始起来了
对那个时候做硬件人都给自己加一个自己的什么PU
但到现在留下来的在这个领域的基本上就是tpu和npu
这两个词比较啊就是比较常见
剩下的就是NPU我理解一下
是不是那个apple 的M系列里面就有NPU吗
他管他叫NPU Neural Processing Unit
然后华为的然后一些比方 韩5G
他们都管他叫NPU
就是他Design Processing Unit for Neural Network
然后Google是不愿意叫他把他写进去的吗
没有没有直接写没有他是用来算就是TPU和NPU他的内核是类似的
TPU是Tensor他处理的是Tensor然后NPU处理的是Neural Network
但Neural Network的底层又还是Tensor
所以这两个东西他只是名字不一样但实际上是一个类似的东西
然后他和GPU的最大的区别就是说首先他是GPU
GPU它为了保证什么
它为了保证 General purpose
它实际上是舍弃了很多性能
比如说我每一个灶
我还是需要算就是提供 General purpose的处理
我还是得算
比方说我得算伏点处理
算高精度小一点后多少位
对吧
我现在算神经网络
不需要那么精度
就是你知道那个图差不多
糊的也能认出来
不糊的也能认出来
所以我就把那个再简单
就是所谓的偷工减料
就是以前那个灶
你得什么吊汤 吊个多少天
现在我不用了,就是撒一刷味精拉倒,就是这个一个道理
它这个变得更简单,然后CMT变成了CMD的话
就是说我不需要让每一个核都去完成相对复杂的东西
我就是完全就是更简单
只是一组数据进来,他给他盖,比方东西进来
像流水线一样盖个章,然后走,下一波进来盖个章
然后走,就更加简单,然后再重点呢,就是他从
那个share memory的模型变成local memory
就以前GPU为了让它高效处理
你每一口灶都可以去冰箱里拿东西
OK
每一个单元都可以拿是吧
也就是那样的话
你对于程序员
对于使用这个人来说
你可以给每一口灶编程
就是说我定义一个
在冰箱里定一个数组
在冰箱里定义一组鸡蛋
你灶一去拿哪个鸡蛋
灶二去拿什么东西
对吧
但是你到了NPU TPU以后
我不要这样子
我就每一个东西有自己local memory
这样就更高效
你就相当于是说你整个那个整个那个台面上你就少了少了很多
少了很多那个互联嘛
你不需要说我这个人要跑到那个
呃冰箱老远拿拿东西
我就直接我这切好了以后往往旁边一扔
然后旁边那个人切好了再往旁边一扔
听上去 应该会丧失很多灵活性吧
非常非常 对 丧失了很多灵活性
所以他就叫 domain specific processing
比如说TPU TPU的处理的精度是tensor
它是用来处理tensor
然后并且它那个core的
它的core的就是运算的精度是降低的
它不用处理运算精度的tensor
它只能处理一个特定大小的tensor
特定大小然后特定
特定大小特定就是数据精度
比如说我们以前是个算一个
算一个叫什么浮点数
就小数点后多少位64位现在我不算
我就小点后就一两位就拉倒了
就能算这样一个东西
所以这导致了它不能够通用计算
这个就是现在主流的TPU和NPU的这么一个
这样一个对比
就是从CPU家庭厨房发展到国宴大师的这种专业厨房
然后又现在到这种预制菜这种乱七八糟中央厨房
预制菜 对
对对预制菜类似的这种
对我觉得听起来有点像是一个深度工业化对吧
就是说这个工业化的程度越来越深
然后分工越来越细
然后我其实在刚刚听你说这个TPU的时候
让我想到了 Giga Factory, 就是ElonMusk做的那个自动工厂
就是很大,很自动化,但只能服务于他自己
对,没错,这个是非常高度定制的,这是Domain Specific Processing
这个这个概念就是在2017年拿了图灵奖
就是那个David Parison和他们,哦,对
就是因为说我们的这个Computer Architecture
有一个变革,之前是大家都讲怎么样把CPU做得更好更快
接下来在2000年,摩尔定律死了以后,摩尔定律死了就是说
我们发现我们很难让CPU做得更快了,就那么大点的距离
摩尔定律死过好多次了,现在还没死
现在基本上就是没有以前发展那么快了,我们读书的时候, 我读书
本科的时候,大家的强调是 post Moore generation,也就是后摩尔时代
后摩尔实在就是我们那个你很难通过skating law去让那个
晶体管做的更小
对 物理到极限了
一到一纳米已经是极限了
三纳米就基本上极限了
三纳米就极限了是吧
再往下
再往下你silicon就漏电了
它那个量子就传效应
这个就不工作了
你想想
想让它更高效的话就要换材料
然后就换材料
就是相当于说你
比方你换那个碳石墨锡啊什么
就是他那一管base那种那种晶体管
他就相当于他在28纳米可以拿到硅 的
那个3纳米的或者7纳米的性能
我们稍微提一点
类似这种就是这种访问
整个产业结构都要改
我觉得成本太高了
对对对对
那个是很难的
那个是很难
所以说我们就 sick for 什么东西
就是domain specific process
就是我做什么计算
我就算什么
我做这个
我挖矿挖比特币
我做一个矿机
对我做这个呃
这个这个这个人工智能处理
我就做这种 tpu npu
然后还有做那个video的处理
我就做那种视觉的引擎
对就是类似这样
这个是现在的一个计算机的趋势
这就是硬件加速
所谓的你有一个特定的任务
然后这个任务你会经常经常发生
所以你定制一款芯片处理这个任务
比如说金融家室类似的
我听下来的感觉是摩尔定律的
狭义的摩尔定律
就是在物理层面上的摩尔定律可能停止了
但是摩尔定律背后的 scaling law 可能在其他的领域还在
怎么说呢
甚至是更快的速度在发生
这个摩尔定律这个词变成了一种共识
变成了一种concept
然后在各个领域去更大的范围的被理解和
我觉得不是定律
因为他肯定是没法计算的
他应该是我们对这个计算能够一直往前走的一种信仰
对的
未来是一个预期,摩尔定律本质上是一个商业定律
就是在那个时候整个半导体70年代是
相当于它这是一个指引前进的方向
你什么都不需要做
你只需要每年把那个面积每两年翻翻倍就行了
两年翻倍两年翻倍
然后现在翻不下去了
可能大家就再会找各种各样的方法去延续那样一个
因为延续那样一个惯性
对,我其实这个事情比较好奇
就是你觉得在背后哈推动这个往前去走的
背后的这个商业的drive是什么
因为芯片是一个周期性比较长的一个这个行业嘛
对吧周期性比较明显
然后最近呢我们肯定是在芯片非常非常好的一个周期里面
然后你能不能简单给我们介绍一下
就是这些计算现在被用在哪些领域
然后这个我们作为普通人怎么来理解他的这个
背后的一些商业价值
现在就是现在要讲就是说这个为什么比如
说这个问题为什么最近这两年AI芯片又重新重新火起来
重新火起来就是由于这个Chat GPT的原因嘛对不对
那么我们可以先回顾一下这个十年前
十年前刚开始AI这个领域刚开始发生的时候
对我们就聊一下AI的这十年
然后这是这是这是2012年2012年2012年的时候
我们是有一个最最早的一个
就是 ImagenNet 那样一个东西
就是说我们是有这样一个举办的
就是Stanford李飞飞教授
他们做了一个 imagenet
那就是一个图像识别的一个竞赛
然后那个时候呢
有一个叫 xnet 的这样一个卷机神经网络
他们通过在 GPGPU上训练
达到了前所未有的识别率
就是在2012年
这基本上是在我认识中这是开启AI时代的一个
对
在GeForce上面
NVIDIA是吧
没错
那个时候是非常弱的一个GPU
一个模型需要部署在好几个GPU上
然后去训练
才能把训练出来
为什么之前没有
这个算法实际上是在 LeCun 1995年97年就提出的解数字
但是那个时候呢他没有办法训练太大了
CPU跑不动
然后GPU大家就是他最创新的点就是
他怎么样去把这东西用GPU给他训出来
然后训出来以后发现诶这个识别率很高
就是说方法就在那
但是因为算力不够
所以没有人能
只有我们等了十几年才有算力
当时那个瓶颈是什么?是工程问题?还是材料问题?还是什么
首先,imageNet这件事情是,第一个瓶颈是数据,就是在
就是研究者做计算机视觉研究者一直在收集数据
在2012年的时候才收集了很多有价值的一个数据集,叫 imageNet
然后基于这个数据集,才能展现出这个算法的一个力量
对然后第二个呢就是说我们在那个时候
大家为了去研究这个算法的极限
自己想我一定要把这个算法这个数据灌进去让他训练
然后CPU跑不动大家就想办法我用GPU去想办法
对就是在非常早期的那个时候我们就在
讨论这个什么神经网络deep neural network
然后deep learning这些词
然后然后那个网络呢也开始从AlexNet
进化成Google的GoogleNet然后到VGG
就是前AI时代,2014年,就是这个时候,大家对AI还没什么认知
直到这个2015年啊,基于那个,基于那个神经网络的AlphaGo起来了
2015年,就我那个时候,就比方说我回忆一下,那个时候刚开始读本科
快,就是我14年上本科,那个时候刚读本科
那两年就是所有的人,学计算机吗?啊,我就学微电子
我是学微电子,我一开始学化学,然后后来就
后来后来就学学微电子
后来我们那个北大的那个信科是可以自己选方向了
你可以选计算机可以选微电子
我们那年微电子只有15个人
就是300个人的
300个人的系里面只有15个选微电子
大家全都去学计算机
全都学人工智能
因为AlphaGo出来
对
有时候AlphaGo是一个基于神经网络的一个副产物
神经网络加上这个
这个这个AlphaGo这个
强化学习这个DQN这个任务
产生了这个神经网络
然后在那个时候呢,那个时候大家就发现意识到这个
训练的这个platform很很很重要,这个infra很重要
因为你就有了这个infra
你才能真的把你的这个算法的这个力量给它展现出来
然后那段时间大家还是在consume这个imagenet
就是对于cv的人来说,计算机视觉人来说
imagenet的魔力还没有被,还没有被这个挖掘透
对,然后为了为了把这个挖掘透,比如说我要看我人工设计的算法
能到多大的识别率,能不能把真人,比真人要强
能不能就是完成比真人还要厉害的一个图像识别
就给你一张图,你识别开始是吗
在我的印象中,那一年正好是第一次AI的爆发
因为我那个时候,我刚刚我还在国内,我刚刚准备来加拿大
但是那个时候因为我在微博,之后那一期我离开的时候就感觉
那一次AI大爆发了,因为就是有很多很多的识别出来
各种识别,然后这种
没错,全是识别,全是因为CNN网络
对,就那一波爆发是大家是认识到了这个很重要
就是什么很重要呢
首先第一个数据,数据很重要
然后第二个就是算法
然后第三个就是算力
算力非常重要
就是你只有算力
好的数据加上算力
你才能真的证明你这个算法好不好用
你不能说我随便CPU训一个10分钟
我说不好用
那就不好用
这个没有说服力了,你必须要要要训个好几天怎么样
甚至上万个成百上千个小时的,在那个时候还没有那么夸张
你才能说这个才能看到这个算算法的上限
所以大家开始build infrastructure
谷歌那个时候大概从14年
AlphaGo就是在那个PPU上训练的
第一代TPU的
对,第一代TPU的
然后Google还有开发 Tensorflow
然后那个meta那边开始Facebook Pytorch
这东西是什么就是infrastructure
就是说你怎么要让那个写代码的人
不要看到硬件
写代码的人不喜欢看硬件
这烦得慌
我不关心你是让哪个灶给我
就烧哪个菜
我不关心你给我烧好就行了
所以说这个infra这个cuda
tensorflow然后codn, pytorch
这些东西就是提供了一个软硬件接口
他让这个软件编程的人就
跟符合他们的习惯
但是他又能高效的部署到硬件上
这个就是那个这些infra的东西的力量
然后那个时候呢大家也开始做NPO
NPO就是比方寒武纪一个著名的Cambricon
开始做他们的NPO
然后很多公司
英伟达也开始做NPO
NVIDIA Deep Learning Accelerator
英伟达深度学习加速器
NVIDIA一开始在做
但是大部分人还是做infrared
training那个时候
并没有那么大的市场,因为第一个,那时候 training
大家也懒了去做芯片, 就因为拿GPU training 就行了
对, 因为你需要 General purpose,它是一个叫什么
那个时候主要还是discriminative的任务
就是说是一个判别式的AM,就是我给你个东西,你看它是什么
那个模型实际上没有那么大,没有那么大
你那个GPU你什么 GeForce 1080,它已经是很厉害的GPU了
已经可以去了,就我记得刚开始搞科研的时候就是
你那个GPU是你可以自己从网上定自己装
然后自己装裤子
所以说现在的很多识别都可以装到非常小的移动设备上面
他的那个识别
没错探测器对吧
对那是tinyML那是下一个时代
这个现在是大家还在探索那个算法
就说那个时候实际上算法没有那么复杂
就是说我们还没有
因为我们现在经常说
哎呀你这个搞科研吗
你学校里搞不了
没有GPU吗训不了吗
你得上公司里做
那个时候还没有那么多怨言
那个时候就是说你自己学校里搭一个杂质的游戏
那时候英伟达股价没有那么高
因为GPU不需要就不是bottleneck
大家没有发现没有觉得是小抱怨一下
就说要有更多就好了
但是没有也可以凑合用
那个时候英伟达还是用挖矿场的
对
然后那时候英伟达还是做Inference
NVDRA是做Inference
他想在GPU里面集成一些Inference的核
然后看看能不能加速一下
对就类似这样子
这15到18年,我可以说他大概意识到infra的力量开始
很多公司开始build的cambricoin, 什么地平线
然后基本上所有的很多公司都会高通苹果,大公司华为
华为达芬奇架构,他开始build自己的这个NPO的这个这个部门
然后再后来19年到22年的主旋律肯定是Covid,对吧
勾起大家的回忆了
然后这个时候大家发现Infra已经Build差不多了
但是在我的认知中它的计算机视觉
并没有特别很好的商业模式变现
在那几年,所以这个热度就会下降下去了
大家感觉好像什么,比如说你云上就是给什么P个图
或者说是阿里那边是识别个图片的那种东西
那个时候变现并没有那么那么那么那么那么
我觉得那个时候中国的设计公司都是to government
对对,to government,比如说安防摄像头那一组
对,然后所以说呢,大家就是对这个的热度会下降
下降了一波,也加上那几年NPO的这个技术卷实在是太快了
所有家的东西都趋同了,因为就那么几招嘛
数据流,然后西书,然后什么低精度,然后这几招
这是一开始你看,就比方说我做学术研究的时候
你在读17年,17年你就读那个文章
你发现,哦,这个很有insight,不同角度的优化
然后到1819年20你发现哎怎么大家做的都一样
所有的 feature 全加进去了
就是所有人的东西都有所有的feature
就变成那样子
趋同了
对 趋同了 趋同
没有什么
这个预示的模型也要趋同了
对
然后呢那个时候那个时代就是
是一个比较低迷的时期
一是因为Covid
第二个是这没有什么技术的爆发
对大家有点像
对有点像黎明前的黑暗
就是我感觉在那个时间点大家的这个这个这个科研
其实也走向两个方向嘛对吧
觉得当时像Hinton他们的研究只是还没有浮出水面
但是已经已经有很多很多人在里面开始发力了
只是还没有做出来一个我们经历过2022年市场暴跌的时候
加息跌的最惨的
类似那个
Transformer BERT NLP
之前都在讨论CV
这就是那三年这三个东西在看上往前走
就是语言,自然语言处理BertTransformer
只要这东西Transformer 2017年就有了
但是一开始大家不是不是很关心
大家都在做计算机视觉
这个东西能往前走到什么程度
没有人没有人太知道
然后然后这个Infra也一直在build
Cuda然后Pytorch这些东西一直在往前往前走
一直在迭代
然后做计算机视觉的人做普通的NPO
发现没有什么就是做的没有那么没有那么好玩吧
就是卖不出去
因为他他他无非就是卖卖语音端和语音和端侧
就是云上就是给服务器上用
服务器上用比如说阿里我做我自己的服务器
我就不断的给别人infrared图片
然后第二个呢就是在手机上
或者说自动驾驶一种端的上面去用
就比如说我在那个自动自动驾驶车里面
我放一个SOC一个系统级的芯片
它可以去识别整个的workload
整个的任务
或者是安防摄像头上的一个处理
然后这些东西呢就是激发了一个TinyML
就是要做那个很小的模型
这也是一段时间一个方向
但是很小的模型它有什么问题
就是它性能肯定会下降
它只能用在特定的场景下
它是一个很小的蛋糕
对
所以那段时间很多公司在做这种很小的模型
就是它模型算法
硬件一起优化
它做那个全套的解决方案
我设计一套算法
然后给这个应用场景设计,他全套都是全自己设计的
厨房我是自己做,菜谱我也是自己定制的
对,肯定他性能会非常好
但是他市场很小,他只能就服了一两个任务
在那个时候,然后加上Covid
大家就整个这个社会也比较
还有很多那个供应链也中断了
对,所以那段时间是比较黑暗,直到就是23年
到救星出现, 对, 就是ChatGPT,两年前的
11月30号,对,就刚好过完两年生日
语言模型从语言模型变成了大语言模型
Language Model to Large Language Model
NLP,然后Bert,主要是GPT的基本的原理叫
Transformer, Transformer 它的模型
然后那篇文章很经典2017年就 attention is all you need
文章 attention is all you need
就是你所有需要的东西就是注意力
对这个好像和我们现在的这个社会的类似
对attention is all you need
放在社交网络里这样成立的
对我后面发现就是这些科研人员
这些实在是太会起名字了
就是论文的就很多论文的这个标题的名字
非常的优秀
我告诉你做科研
做科研跟做媒体一样的
你的目标就是要上头条
没错这个是说的这是说的太对了学术界的学术界的
运行规则 somehow 也是 attention is all you need
attention is all you need甚至说我们之后叫什么起名字
你首先你的文章一定要给你的东西一定要起一个简称
然后那个简称的必须要有一个具体的意义
然后然后你还得符合你的那个
你的特征技术里面那几个词选一个你
起不出来好像就不是一篇好文章一样
这个是这种感觉,现在,然后,然后这个chatGPT
它和之前的主要区别是什么
这个Large Language Model,它就选择Mix of Expert这模型
我不确定现在 OpenAI 上用的是什么
但是第一个大模型是 Mix of Expert
就是Moe混合专家模型,它把很多很多小的Transformer放在一起
相当于是,它那个,就是比如说我有一百个专家
然后我一张一个token过来
我找某个先预打分
然后找打分的那个专家最高的那个
让他去做influence
也就是说他这个他这种mov模型
把那个模型横向扩大了非常多倍
但是纵向没有扩大
也就是说
他需要把这个模型的这个这个
模型的运算运算的量没有增加
但那个模型本身变大很多
他需要存储需要通信这个代价变高
嗯
对这个就导致了说这个训练
这件事情变得没有那么容易了
不是我以前从淘宝上订上两张卡
插上 Cuda 一装
我能训了
以前是这样子
以前一开始那个那个就是随便订订两张卡
然后然后甚至是你训完以后还可以打游戏
是吧
就是完全是一样
现在那不行
这个模型变得很复杂
你需要有一个集群
你需要有一个专业的这么一个
甚至甚至不仅是就是不仅是专业
专业的机房甚至是还找很多人管理这个东西
而且那个info在前几年的build的过程中
它也跟上来了
就是我刚开始做的时候
你刚开始学的时候
你那个PyTorch和那个Zerf里面全是bug
就是你自己写完文章里面一堆bug
你还给它debug
你用那种东西
根本就不能想象它怎么样去
我同时操纵多个GPU
同时操纵一个超大的集群
用那个东西完全不敢想象
但因为这两年
他他做他给那个推荐系统用的时候
他把那个 infra build 的起来
嗯就是 Meta 他们做推荐系统
推荐系统也是一个神经网络
他也让这两年就是在疫情期间吧
还买了大量这个GPU
不过嘛这个我都推荐的
结果正好碰上了这个 Chatgpt出 现
结果一下就找到GPU了
Meta就说狗屎运
对这个GPU
GPU就用起来了
这个chatgpt 必须要用那个
GPU 训为什么呢
因为GPU可以提供 general purpose的处理
OK
我在模型没有固定的情况下
我拿一个都没specific的东西去处理是没有意义的
因为我都不知道它需要算的什么
那有问题
那这里那tpu呢
现在你google的产品训练模型的时候
这个限制是什么
对于对于GPU来说
做训练的时候
你tpu吗
嗯对
比如说tpu它高效的处理只有那那么几个类似的
只有几个类似的这样一个算子
那么你在设计算法的时候
你就相对于会倾向于用那些算子
那你不用那些算子的算法
跑了就没有那么高效
在设计上说就受限制
但是通用的是吧
也是通用的
GPU是通用的
对TPU是通用的
是有通用的
有很多版本
TPU很多版本
有专门做Inference的
有专门给大模型的
有专门训练的
对 但是他训练的那个训练的那个部分
因为没有GPU那么GP,没有GPU那么GNORMAL
所以说,GPU在大模型训练这个任务,就是说生成式AI
我们把大模型扩展为GenerativeAI,生成式AI的训练这个任务中
GPU是不可或缺的
对,你我能不能这么理解
就是基本上只要你是在做training对吧
在pretraining这个部分里面
GPU的位置确实是独一无二的
现在是这样子的
因为那个算法不固定
等那个算法固定了
那你就可以在customizechip去train它
对没有关系
然后今天能做generalpurpose的训练的
也只有英伟达的GPU
也没有其他的产品的替代
我们那也可以
谷歌的这个谷歌的这个TPU也可以
但是它不往外卖
然后AMD也可以
但AMD的Infra不好
Bug太多
我有听过AMD的一个八卦
就说AMD其实也去Pitch过一些大公司
然后就说什么可以有更便宜的价格
我有更多的工程师来帮你
但是实际上在最后的转化当中
大家很难做决定
因为它的稳定性实在是差太多了
AMD的性能纸面性能
据我所知是比英伟达好的
AMD硬件做的很好
什么叫纸面性能
就是我能够确定
纸面性能是什么
纸面性能就是tflops
比如说那个我算力
我把芯片拉满了
如果BUG的时候
那它马力全开
我这些就是
对吧
迷小气泡
700匹
直接0百加速3秒
对吧
但是你真正开的时候
用的时候
你发现它全是烂路
它加不起来那个速
就是BUG特别多
这个我也有问题
就它有这么多BUG的原因
会是什么
因为他的Infrabuild的时候
在前几年这一圈时间这Pytorch这些东西
都是基于英伟达的Cuda去build的
在你看这15年开始做的东西15到18
19 22这小六年多的生态发展了
对都是基于Cuda去build的那个东西的
也就是说那个时候我们可以想象
你说你想讯息网络先买英伟达的GPU
然后装Cuda装CUDN才能用Tensorflow
或者是pytorch
AMD那块怎么做网上连教程都没有
对所以我能不能这么来理解
就是最终的训练效果
它是一个软硬结合的一个事情
然后软件和硬件都很重要
然后AMD它可能输在两个方面
第一个是它虽然硬件说它的纸面性能那么高
但它由于产量没有英伟达那么大
所以它本身的硬件的稳定性
我自己觉得可能就不如英伟达那么好
第二个呢是由于它没有这个软件
所以它这个软硬的配合就一直都一般
所以在这个过程当中对吧
不管是这个数据和数据之间的传输
还是这个比较偏这个这个这个之间的连接
所以都不会很好
都会更大的概率出现bug
我这么理解对吗
第二点是更重要的
第一点说它的硬件稳定性和英伟达
这个不好说
AMD的东西是挺好
就是说它软硬硬件实际上不好用
不好用不是说它本身不稳定
它挺稳定它不好用
OK然后这个不好用
想变成好用是不是唯一的办法
就是更多的人来用
有一个很好的社区更多人来用
然后report bug
然后有人解决这个问题
然后不断的去维护
然后那个版本从1.02.03.0一直在迭代
迭到最后形成一个区域稳定的版本
这就是GPU和Cuda Pytorch这些东西的一个交互
就是说这些十多年的积累不是一个公司想通过
对这不是公司能搞定的
这是生态
比如说Pytorch是Meta开发的
Meta开发的Pytorch是基于NVIDIA的Cuda
它的backend是Cuda
对这两个是联合在一起的
Google的
Google是Tensorflow
Google是Tensorflow另一套
Google是自己的,全是自己的Tensorflow
下面有自己的Cuda嗎?但是它也可以在Cuda上使用嗎
Google不是,Cuda是CustomizeforGPU的
Google是直接Tensorflow接到TPU上
這也是Google這十年的生態
這兩個生態已經帶位置
所以我能這麼理解嗎?就是說現在肯定
最大的生態是NVIDIA的Cuda的生態
但是Google呢,由於自己有這個TPU在跑
所以至少這個TPU是有生態的
然后其他的人都barely能说自己有生态
对吧没错没错
就是说可以甚至大言不惭的说
这个真正能用上真正能用起来的
只有英伟大和谷歌
真正能用起来能用来处理模型的
但谷歌又不给别人用
所以谷歌自己不够用
然后我的自己就不够用
那我正好还有一个问题
因为刚才你的历史讲的很好
对我也学了很多的CPUGPU
然后后面的整个这些现在这么多年的AI的发展
怎么在这些硬件技术上成长起来的
可是另外一个非常好的视角
正好顺着这个
现在我们就前一个月吧
很多媒体都报道了什么那个训练撞墙了
然后摩尔定力
在这个时代是给你诺会失效的
但是我在上两周
杰森黄接受我忘了那个 No Priors 的采访
他们那个podcast
那个我印象很深刻
我听了一次他说
NVIDIA未来十年的计划他也透露出来
每年要把这些性能提升两到三倍
现在又要进入了一个超摩尔定律时代
对
超 摩尔定律
然后他具体讲的就是说
数据中心集产品
你要把一个数据中心
就是一个大的一个集产单元
然后要把这个里面装上
比如说3万块10万块
或者物理的里面要限定
我不让装100万的GPU 串联起来吧
唯一限制的就是网速
你们怎么看这个事情
就我感受NVIDIA已经并不是一个硬件公司
它是一个软件公司
甚至说它是一个AI公司
他说把这个算力往上去增加
指数级的增加
它并不是说增加一块芯片的这个效应力
它是说把这个系统集群build的更大
用的够大
然后它解决芯片和芯片这样互联的问题
相当于造一个AI时代的超级计算机
这NVLink它的核心技术
对
NVLink是一个带宽的优化
就是说我以前是说我一块chip
一块chip它里面怎么让跑的更快
现在说十几个chip
甚至成百上千整
放在一起怎么跑得更快
那这个跑得更快你会发现它的那个
那个包容那个已经并不是说里面怎么弄
它就是说芯片和芯片之间互联怎么弄
就是这种这样一个问题
就它的优化能超越了硬件本身到更上层的
就听上去因为我看过发布会上讲过
整个数据中心就是一个GP
对没错
就是这就是一个
这就是一个那个Infra的一个力量
就是 Software Calibrate Interface
它可以讓它可以讓軟件寫軟件的人把它看成一個GPU
而不是說我先先寫一句話
你GPU一你去幹什麼
GPU二你幹什麼
幹完以後你一從二拿數據
拿完了以後你再給我怎麼樣
這樣這樣對於寫軟件的人來說他會很痛苦
嗯我我自己的感受就是英伟达的這個這個競爭力啊
就是從這個微觀走向了宏觀
就從微觀裡面最開始這個設計這個小芯片的結構
到今天這個我我自己覺得
他可能是这个全行业少数的能够
可能是唯一一个吧
能支持做10万卡的集群
甚至马上做百万卡
就我感觉
对我自己感觉10万卡会很快成为标配
因为 xAI 做出来了
会很快成为标配
然后背后的这个交付方呢
就是英伟达嘛
所以那至少肉眼可见
英伟达在未来的比较短的一段时间里面
要给10家左右这样的公司交付10万卡
他说了后面
因为现在这个也有需求嘛
这个也有需求
呃
就是说大模型的魔力在网上能到什么程度
现在好像是被算力绷得住了
我们不知道你把他10万卡变100万卡以后
那模型会变成什么样子
对不知道
所有人都想要带这个10万卡训练出来的group3
对啊
所有人想知道会变成什么样子
所以说这个
这些做硬件的东西就会去卯着劲去去进入这个问题
所以我其实想知道现在至少在GPU这个方向
它是一个叫什么战略是明的就像Indigo说的
它实力要做的事情其实说得也比较清楚
与此同时TPU也有做训练的能力
然后今天的战争从这个微观
刚刚我们说走到第二阶段生态
甚至走到第三个阶段宏观
那就是tpu的下一步
就大概啊
你觉得从公司那边来看会会怎么来做
呃
就是首先是谷歌
谷歌作为一个互联网公司
它实际上它并不是一个芯片公司的文化
它是一个互联网公司
一个软件公司的文化
它这边做的整个infrastructure的目的是为了摆脱
NVIDIA的控制
就是做自己的
做自己的东西
对
所以它肯定会它它是一个就是
内部是有很多的
很多的这个很 aggressive的一些plans往前走
就说我们要做更好的这样一个推理
然后推理训练什么各种各样的一个芯片
都是在做的
然后就是他并不会只把目光放在训练上
推理也会做
因为他要serve自己的模型
他正好问了一个问题
我来问吧
我来问这个问题
关于推理的
或者在推理之前我还有一个问题
现在有那个怎么说呢
因为我刚才说到了NVIDIA的解决方案
它把整个机房变成一个大的GPU
一个GPU
然后呢内存的数据还是跟计算分开的
它只是快速传输
现在有一种方案
比如说 Cerebrus
或者说是类似于Tesla的Dojo里面用的D1芯片
他们好像是把成算放在一块
然后内存离计算特别近
然后芯片做的特别大
对这种方案觉得是它是能够来完成特定任务的
还是觉得会成为一种新的竞争
就存在就算了
EMEM COMPUTION
我老本行 EMEM COMPUTION 是分为很多种
对
SERVAS 是个什么叫 Wafer SCALE COMPUTION
就是WSE
就是 Wafer SCALE ENGINE
就是整个大的芯片全都是一个大的chip
然后上面就是一个芯片上就是一个数据中心
这个是一个我比较看好能够有颠覆性传统计算模式的一个东西
Wafer Scale Computing
然后如果是比方 Groq 的那种纯粒计算的话
我个人认为是差点意思
这个纯算的做的比较好的Cerberus
Cerberus不太算纯算
它是一个更多的是一个Wafer Scale Computing
OK那那个呢
Tesla Dojo了
那个里面用的芯片的架构
呃那个那个就是那个更多的感觉更像就是 local memory的一个support
哦 local memory是吧
我我我不是因为他是传统的存算
因为他他就是说在片上放了更多的memory
而不是说让memory去有计算能力
OKOK那我可能那我刚才可能说错了
刚才说的我只是说那个片上面那个内存
你芯片计算单元更近
他刚才说的存算是一体的不一样
但但我做到现在我的感受是说
现在更重要的是软硬件的那个交互
结合是吧
你那个core 是怎么算的
实际上没有那么重要
我现在感受到没有那么重要
它infrastructure
比如说我做硬件
我给提升了90%
但是你软件跟不上
你提升东西你用不起来
你芯片在空转
它不转
它在等别的东西
对了
它在等程序员给它编程
就是苹果硬件的问题
苹果硬件太领先
然后软件不行现在
所以未来就是要software hardware code design
这件事很重要 code design 非常重要
尤其是对于这个对于这个 serve inference 来说
就我模型已经差不多固定好了我模型已经固定好了
我需要去高效的去serve它然后我在数据中心上serve
我去机器人上那种就是candidate一点的serve
我那个模型会通过模型加速的方式变得很小
比如说我降低它的运算精度我给它做吸收
他算力需求变小
然后并且这种模型还会根据硬件
定制一个自己的处理的这样一个系统
这个是Inference的未来
所以Inference这个里面会有好多不一样的机会
Inference 的机会有非常多
但T raining 现在看到的是需要general purpose 处理的
就是能在市面上给大家卖的
这个我已经听到 LeCun 最新的podcast已经讲了
他说不要和英伟达在training上面竞争了
对对因为你别做出来东西
然后就是你给你给别人提供说我这有一个啊
便宜便宜100倍但是不好用
算的不准能算快但是不准
Inference上现在是什么格局啊
Inference上我
如果是 specific LRM的话
我感觉现在全能提供 LM Inference服务的公司也没有那么多
然后很多芯片公司它都在支持自己的Inference
就是LM的Inference
比如说那个Roc
这是他们在做自己的Inference的这个platform
但是说到底有没有模型的公司去用它
这个不好说
这个不好说,因为我个人认为将来做Inference
强的这个公司,一定是做模型的公司,你没有模型,
你做硬件,然后别人说你在我这地方跑,那这个一定
不是最高效的,要高效一定要software hardware code design
那我听上去Google还是有机会的是吧
Google就互联网公司 Google Meta 都是有机会
对都是有机会 Google 和 Meta 也在做自己的chip
然后OpenAI也在做自己的chip
对OpenAI最近你看它 Open AI Career找很多senior的人
这是另一个有意思的地方就是OpenAI只找senior
为什么因为OpenAI跟他们内部的人聊天
就发现就是说OpenAI的人他发现
大语言模型可以帮助 chip design,不需要 junior 的
你一个senior的,以前是一个senior领着,比如说我define一下architecture
然后下面招集一堆小兵去帮我,你写这个模范,你写这个模范,
然后openAI直接拿 chatgpt,让chat 就写出来了
我不需要是那个小兵,就是不需要招那小兵,
只需要一个说你写这个,你写这个就可以了
对, 这个正好, 你说到这个话题我问一下,因为我也看到
就是GoogleDeepMind有论文吗,说你们现在的
Google的TPU现在越来越多的采用了AI设计了
它是有很多部分嘛,就是刚刚说设计师说前端就是
Logic Design,就是你要告诉那个你去Define 那个芯片有什么功能
然后后后端的那些部分,就是比如说你
怎么样去把那个功能变成芯片的layout图纸
然后交给那个台积电或者交给什么
fabrication的人给你做,这这两个部分是不一样的
AI做的哪一部分,说的是我们现在就是我认为这个AI比较容易做的是第一部分
布线,对,先做那个 logic design,你AI比较擅长处理这个逻辑的东西
就是说动脑子的东西和physical相关的东西,AI还是弱一些
然后这是第一个就是,而且如果是说想要去把这个东西做出来的话
我个人感觉,英伟达也是很有机会的
因为他里面可能有很多人在做这个事情
英伟达的优势是什么
英伟达他有数据
因为他有很多数据
他他已经设计了30年芯片了
他有芯片设计的数据
他有大量的数据积累经验
其实这个low耗别人不知道
他没法训练
对他可以把这些数据喂给那个模型
他这是比Open AI厉害的地方
OpenAI 有很好的大模型
但是他没有很好的数据
我感觉再往后进化两年
原模型能够设计运行自己的东西
设计运行自己的硬件,他来改进自己
这个我不认为是一个不可能的事情
很严谨的一个回答
这个事情很保证,因为它并没有那么难
我不认为我的工作被AI取代不了
最好是我想跟着一个话题,在这种情况下你怎么办
我在这样一个时代,我作为一个博士,博士毕业生,我的工作能为AI取代
那这个时代是人类一个很伟大的时代,你都这时候你还琢磨你明天吃什么
你工资会不会被替代,那太没意思了,对不对,对,你可以抢Indigo的饭碗 做博主
这是一个伟大人类的变革,因为你想你作为一个博士生
你的工作都可以被AI替代
那这个人类文明是不是已经到了一个全所未有的地步
已经站在了一个历史的巅峰上
然后这个时候你还琢磨明天公司帐上多一零少一零的事
没必要
基本上就所有智能任务AI都可以完成
而且我觉得这个生产力会就是按照机器的扩展
我只要有能量有能够生产GPU
我就能够扩展机器
对,并且这个是training的training的一波的这个这个然后这个机会
但training完了以后serve machine learning 的serv e一定是继续下一波机会
就是说我怎么样去把这个功耗给它打下来
我怎么把这ChatGPT做手机里的ChatGP做机器人上
embody AI 嘛,就是说 physical
而不是说一定要所有东西跑到服务器上去算一下
我拉到我本地去算功耗
而且我觉得对大量的这个可能90%多的这种需求都是在本地计算
百分之对没错没错
这是为什么一开始做那个cv的那个任务的时候大所有工作跳来
做inference 第一个是简单第二个是确实他市场需要打
嗯对就是做training的就是让英伟达做去吧别人我不管不抢人蛋糕
就做做inverse对我们 寒武纪 啊什么我就做inverse
地平线然后那个深剑科技当年在PG上做那个
就是按方向头的那个端的一个Inference
都是做这样的模式
我听上去Inference的格局会不一样
会和之前因为
推理训练基本上是一家垄断
因为它有历史积累
有很多很多社区成立这么多年
对这个问题
大家可以把这个TipTube关掉的声音
然后在推理上面
而且我听上去和软件关系更大
就是说哪一个模型
他有自己的需求
他就会设计推理
比如说特斯拉特斯拉当他的optimus
他现在用的AMD芯片他肯定要做自己芯片
没错又得做自己芯片
因为他那么多车而且用的是同样的推理结构
他和 optimus 机器人对吧
特斯拉是有做芯片的传统的
他之前做过很多chip
所以说他一定是由自己的部门去做这件事情
而且自己做的好处就是刚才说的codedesign
对于这种寸土寸金的功耗寸金的地方来说
他必须要把那个功耗压到极致
就是说我一点点冗余都不行
我的算法必须高效
且只能高效的跑在我这个chip上
对我刚刚听麒麟在想
他说这个软硬一起的能力特别重要
那我也会在想
就是Inference如果能出新的机会
会是什么样的公司能摘到果实
听起来有可能是像Tesla、Meta这种
做应用
对做应用
用最大场景
就在前段的公司能摘到这个果实,对吧
他知道算法要干什么,他知道什么样的算法好用
然后然后再定制硬件,这个这个是这个是叫什么,我想想啊
从九十年代到现在的,以前是说做硬件的
就是说你别管你要算什么,我给你做的更快就行了
我们就聊那个什么每秒钟算多少次加法,就聊这个现在这个格局就变了
格局变就是说我告诉你我算什么
你给我做一个算的更快的东西
对变成这样子
而且我觉得正好我们话题还没问
就是在edge端就是在侧端
中文叫侧端英文叫edge端边缘端
这种场景我觉得会对你说的能耗
速度软硬度非常高要求更高
我们可以想想未来在edge端
要用的这些芯片是什么呢
首先我们所有智能设备
已经被两家东西
对于Google还有苹果
是吧这两家
然后剩下的就是智能穿在身上的东西
什么手表苹果
现在我觉得下一波很多人要竞争
就是这个眼镜
眼镜
这个里面会有巨大的
没错
所以说这应该是下一波最重要的一个
在edge端的一个突破口
没错
眼镜
但这一波的机会
我不认为是
我不认为是传统硬件公司可以抓住的
如果他不知道他们要算什么
他做东西没人用
那我觉得MetaApple
MetaApple不算传统硬件公司
Meta是
我说的机会可能在MetaApple上面
包括高通他可能现在执行
帮Meta定制
但是Meta以后就直接我自己做了
Meta是有很多做芯片的部门的
我们可以从 career job post上看出来,他们会做自己的,一个是ARVR,
然后第二个是做推荐系统,一个云,
一个端,一个cloud,一个edge,他们也是在端子
但是我觉得Edge端芯片的突破才能更有机会带来
更多人去使用到一些AIEmpower的硬件
它不可能这个能力只在Apple和Meta手上
这个能力最后一定要泛化才能够进入到更多普通人的生活
就是说等一个模型变得更简单了以后
或者说开源的更多了以后
那么硬件厂商也可以去做
它模型边开源了opensource
我硬件厂商我可以拿来模型去训练一下
我自己提供我自己的东西
那硬件厂商就像这样
硬件厂商不是个硬件厂商
它就变成一个像软件厂商
那现在那个Groq不就是在跑Llama嘛对吧
对Groq那Llama是Meta做的嘛
所以说
Meta的时候还是给Meta打工
对
对其实类似这种
在这个时代谁控制软件谁控制应用
对谁控制在上面它和以前的不一样
它和九十年代最后一张图这样放的这个
我给你放上来
就是说它和我们九十年代那个PC的那个逻辑是不一样的
那个时候Windows和Intel两个一联合
我不管你们做应用的是什么
你们必须可着我的这个操作系统来
你必须可着我的硬件来
你在我搜索上跑不了
你在英特尔上跑不了你
你东西没人买
现在不是了
现在反过来了
现在我的硬件
我硬件唯唯诺诺的
我得跑你的模型
我得跑Lama
我得跑什么
跑不了
没人买
对
是的
对
Yesterday once more
对
对 我的日常
就回顾一下90年代到现在
我现在感觉到现在是AI的一个什么时代
就是 Infrastructure build 的一个时代
就是像
还有build的Infrastructure
对
那90年代PC,慢慢做那个chip和操作系统,这两个做起来了以后啊
做起来了以后,然后再再person person compare
那些应用才慢慢起来了嘛,对吧
嗯,然后像互联网时代一开始造基站,造那个
造那些就是协议,互联网协议,对
我小时候2000年初那个时候,谷歌百度什么4399那些东西,163门户网
然后一开始是那些东西,大家就渐渐熟悉互联网
上网还得打电话拨号嘛,对不对,一开始,后来才有博客
土豆网,优酷,这些都还记得啊,六间房,还是去美国时间
不够长,经历了比较完整的中国互联网时代
到了08年,08年那个时候,然后互联网的应用才起来,微博,但那个时候应用也也
没有那么赚钱感觉,后来投资京东微博,甚至是10年代开始起来的东西,然后10年代
我感受是最大的就是移动互联网, Internet of Things
万物智能设备,万物互联,对啊,手机在我08年
一经历年上初中,初中的时候,就在看iPhone3,iPhone4
那时候,对刚开始,但是一开始大家也不知道怎么做
玩,所以我们这是一个能玩水果忍者的一个电话,对吧,然后TikTok
像这种移动互联网时代的 killer application
也是在20年,10年之后有的,对,也就是说现在
我们出现AI可能大家在认知就是一个
Somehow十年前能玩水果忍者的一个TikTok一个手机
对我现在好像还没有出现水果忍者
对现在连水果忍者都没有
现在操作系统ASI操作系统都不知道
就是操作系统我理解为就是说人机交互的一个
一个媒介
就比如说Windows
Windows它的最大的变革是Gui嘛对不对
是用鼠标用键盘去点而不是打字
像以前对啊
然后苹果的移动互联网就是用触摸屏去接触这个世界
而不是要跑到网吧里去看电脑
对
AI时代的怎么接触
可能是眼镜
可能是项链
可能是什么
我觉得可能眼镜是一个非常重要的一个
对眼镜是
就是我视觉和语音的交互
但我的眼睛不习惯
有人就不习惯
到时候你自然会习惯的
到时候不得不习惯
你拿手机习惯吗
肯定不喜欢,现在自然喜欢
现在甚至连操作系统都没有
那么将来killerapplication就更farawayfromthatway
所以说AI有很长的路要走
你刚才讲的这个挺好
就是说我们现在还处于AIInfra的初期阶段
Infra都还不成熟
现在我们推理还很混乱
每家都有自己
训练值算被统一了
训练值算因为现在的
英伟达的绝对实力把它统一了
但是在推理这一端其实还好
还有好多机会
而且我感觉是一个软件驱动硬件的机会
对
而且现在训练
它主要是说你要模型要改
如果以后模型不改了
只是说我给它灌数据
给它反听你那个模型
就是我模型就那么模型
但我想它更聪明
我喂别的数据
就是说模型定下来了
算什么都定下来了
那这个时候也可以定制训练芯片
甚至也可以定制训练芯片
Post这个training的芯片,对,所以我们去看未来有几个关键的问题
可能我们要我们要去讨论,第一个就是模型在什么时候可能会定下来,我自己的
我自己的感觉就是这个不到百万卡是不要想这个模型
能定下来的,就是人类一定会冲百万卡集群
模型一定越来越大,但是他希望把这个全世界所有的数据
全都给他灌进去,对,那么第二件事情,那个那个
Anthropic CEO Dario 他讲的是如果说我们到了一千亿美金来建立
所以训练中心的训练的时候
在我后面在建的时候就编辑效应递减了
可能在这之前就是你说的那个时间节点就是在做一千亿
那就是说一百万一张卡以上或者三百万卡连起来训练
那大概是什么时间呢
2027年我们能看到千亿美金的集群
对千亿美金集群
那OpenAI得融多少钱啊
哈哈哈我自己也来做自己的芯片
这是个很很很了不起的伟大
就是当年上月球一样
你就是人为了上月球
完成那样那个航天技术
在迭代式的往前走
我我我不管上去又会怎么样
我就是想看看
我想看看到了那100万以后
那个模型会变成什么样子
能就这种好奇心驱使人往前走
现在就是我觉得2027年会发生两件事情
一个是可能2027年或者2028年
我们会发现千亿美金的这个集群诞生
然后人类有可能会上火星
两件事发生在这时代真的是
这是我对刚才正好李厚明说有一个很要问题的问题
对吧这个时代参考什么时代
对对我先问我第二个问题
第二个问题就是说千亿美金之后
大模型的这个事情格局能看清楚吗
就是在inference这个领域大概将来会是一个什么格局
我们知道的确定性的拿到门票的玩家是
是meta是apple对吧
是一些比如说像google那tier2是创业公司
还是现在的像什么高通这类的公司
还有机会就你怎么来看
就是在这个这个这个端侧或者是推理这个领域
很复杂啊
这个我我我个人感受到现在如果没有大模型
的话就很难往前进场
对暂时还定不下来
对其林的意思说公司手上没有模型
那就没有门票
对啊现在我觉得很难
对这很现实
那就现在我们掰开手指头数都数的出来谁手上有模型
但是你提供infra
你还是有蛋糕可以分的
比如说你要做互联
你要做互联
Broadcom它提供高速的switch
对你要做互联
你也离不了它的光通性的那些东西
对吧
然后高通他自己就是 callit communication
他自己擅长做通信
NVLink他里面那些核心技术
他也可以去分一杯梗
对
我们其实还有一个传统的硬件公司
他更提供的是硬件里面的一些
就是核心的设备
像这样就像做发动机里面的某个零件一样
对但他不直接造机
对这个其实有点像车对吧
就是上一代这些造车的人都以为电车就是三电系统
没想到等这个电车真的来了以后
三电系统是里面最简单了
其实是自动驾驶是数据是电池
对
对所以我觉得算是一个划时代的一个东西
哎正好正好
有模型的就ok
就是模型就是门票
而且模型还有两个档次的
Tier 1 Tier 2
现在有模型他就可以迭代
现在如果没有模型
他从零 build的, 主要是他从零build的,我个人感受他
他步步付出,他没有成本,他得烧多少钱才能干这个事
没有钱,我觉得这个这个这第一场战已经结束了,就这么些公司
这个这个的入场时间已经结束了,诶,正好这个问题我也想
问问indigo啊,因为你这个在互联网的时间很长
对吧,而且你现在又是一个这个科技博主,知识博主
就是今天我觉得在这个AI领域,尤其是在AI这个infra这个领域
的竞争格局是非常有意思的
就是基本上NVIDIA处在一个垄断性的
unbeatable的无法挑战的一个霸主地位
然后从你的角度来看
你觉得历史上有哪一些时期我们可以参考
以及你怎么来看
就是这个竞争态势的一个变化趋势
未来十年会怎么变
历史上像这样的绝对垄断好像PC时代都没有诞生过的
它只是IBM兼容机这个协议垄断了是吧
IBM和Intel的那种感受
对那个是IBM加Intel的垄断
但是它也没有像NVIDIA这么厉害把训练垄断到滴水不漏
一次
然后再往前走再往历史再往前推呢
100年前那個時候我想想看在做什麼
電力供應也沒有
然後那個時候IBM商用打卡機是壟斷的
很厲害
那個年代
你看那個計算機博物館的時候
最開始展出的都是IBM打卡機
那個是壟斷的很厲害
但是再往前面走了可能因為
人類還沒有那麼多集中生產
全球化還沒那麼大的
規模還是很分散的
所以說沒有那麼大壟斷的公司出現
現在是因為高度全球化之後
三十年高度全球化
大家的組裝啊
那什麼東西
這個都
而且還有互聯網
所以說這個他壟斷起來會比以前更快
現在現在硬件
你看軟件公司壟斷其實更厲害
對比硬件
他就被平台的選擇
你沒有選擇是吧
說是我就是誰的
然後社交網絡就是誰的
你根本就沒有選擇
你沒法選
就這一家
所以說訓練就這一家
這一點
就我我我認為是
是這個時代造就的
而且他有他的機遇
但Nviadia的特点就是每一个阶段的时候
它总能趁着热点
先开始做游戏加速
然后挖矿
然后AI
然后它什么
接下来它可能还有一波机会
除了AI训练之外
它的模拟
它的simulation
也是靠GPU的
而且你看这种空间智能
simulation
它还是有机会的
英伟达起家的是graphic
他又回到他的原本他又他又轮回回去了
他本来在做渲染的将来做那个AR V英伟达
垄断了计算
Computing这个概念
而且是规模化的不是说终端上的计算
那种是规模化的
当我要把计算轮起来的时候
所谓是一个超大计算的时候
他就把计算轮到这个
他是一家软件公司
他也是一家平台公司
黄老板自己说的,NVIDIA它是基础设施未来的,它就是基础设施
fundamentally就是一个东西,对,这个是基础设施,所以感觉
感觉听起来就是至少未来的很长一段时间
对吧,如果打破不了英伟达的股段
只有它股票,哎,量子计算会是吗?量子计算
我个人感受,它需要,它的条件太严苛了
你得关在那个什么那个温度啊
这什么东西那零下就是不是零下
就是尼尔尼尔的好观众了
对啊对啊
而且他算的就那么那么一个一点点
而且最主要的是量子计算
他的吞吐量是不可能上去的
对啊
这是什么吞吐什么叫吞吐量
就是你需要先把数据变成他能认识的形态
然后他给你算算完以后呢
你再把那个数据拿出来
嗯你你变和拿这两个过程就已经
还有一种这个注意一下最近亨腾教授在外面所有演讲上面讲的这个仿生的计算
就是不是像现在的的摩托的对
Neuromorphic 那个 Neuromorphic 有点难做
就IBM的 True North 加上那个Intel的Loyalty
一开始在大约是在1718年那个时候
大家都觉得 Neuromorphic 是Promising
因为觉得NPO比如说矩阵乘法吧
矩阵乘法有什么未来吗
我们需要研究那个人的生物怎么样走
在那个时候那个是很promising的
但是后来被大毛型打得出力了
他讲的可能还不是这个
他讲的可能更接近于这个生物结构的
