好的，我已經將文稿整理如下，主要針對口語化、重複、冗餘的詞句進行了刪減和潤飾，並保留了原文的重點和對話的流暢性。

**整理後文稿：**

大家好，歡迎回到 Indigo Talk。

今天我邀請了我的朋友藏师傅，他之前叫推友。我以前關注他的AI newsletter，慢慢注意到他，後來加了X和微信，成了網友。今天邀請藏师傅來跟大家分享超級個體這個話題。

因為有了AI，可以給個人賦能。藏师傅會來分享超級個體的内容。之前我也邀請過創投、Web3、加密貨幣等不同領域的朋友。上一次聊了比特幣，不到一個月ETF就通過了。今天第一次邀請AI領域的專家，不聊創業投資，從個人效率開始，聊如何用AI賦能自己，將手頭的工具變成生產力，提高工作效率，做好更多作品。

藏师傅先自我介紹一下吧。

**藏师傅：** 大家好，主要關注AI，側重生成、圖像和影片。大語言模型只懂一點點，算個菜鳥。主要在推特、即刻和微博活動。微博有些東西大家都懂的，能發的發，不能發不發。

**主持人：** 了解。今天我們設備很齊全。我們會將節目錄成播客，沒趕上直播的朋友可以回頭收聽，但建議大家看直播更有趣。今天的主題是藏师傅在X上分享最多的生成式圖像和影片。我今天的直播背景就是用藏师傅的提示詞生成的。之前預播的影片也是藏师傅做的。

第一個話題，我們聊聊生成式AI和圖像相關的。大家可能都用過Midjourney，想听藏师傅從個人角度分享一下用過的開源或Midjourney、Adobe等生成工具的判斷和特色。

**藏师傅：** 圖像方面，不開源的就Dalle、Midjourney和Firefly。可能還有一些國內的 Leonardo AI。

Dalle的優勢是很多畫不出來的它能畫出來，但不太好看。他們為了多模態，隨手搞了一個東西出來。他們在這方面的經歷不多，只是為了配合GPT的多模態。他們可能會轉向影片。相當長一段時間內，Dalle3會是這個樣子。

Midjourney剛出V6，提示詞理解和質量都上去了，現在是最好的，毋庸置疑。但有一些版權問題。

Firefly是Adobe必須要有的。它的能力比較大，但不是以AI為主的公司，迭代可能慢一些，但積累是有的，追上來很快。但在語言模型疊加和提示詞理解方面可能會慢一些。

SD的話，入門門檻高，特別是最近ControlNet出來後。雖然更新快，但交互奇葩，插件名稱互斥，不做測試，對新手入門造成困難。WebUI已經非常成熟穩定。但如果Midjourney的一致性OK，除了要做相關產品的人，其他可能慢慢會下降。它兩頭不太夠，既沒有那麼好用，也沒有那麼靈活方便。

**主持人：** 了解。我們聊到了Dalle、Midjourney、Firefly和SD。我一般做播客節目會做個科普，因為有人沒用過。

我會把邀請嘉賓的話題不間斷回顧一下，讓後面進來的人聽得清楚。

藏师傅講了圖像生成，最重要的Dalle、Midjourney和Adobe的Firefly。Adobe的產品已經融入到各子產品線了。我自己設計小白，用Photoshop的 generative fill 生成、摳圖、補背景、延展圖片大小特別好用。Adobe Express裡面也有很多集成的工具。

SD我是最早用的，2022年很激動，覺得AI變革時間到了，被SD驅動。當時拿自己照片訓練畫自己，雖然畫得很糟糕，但自己學習了神經網路，理解了分類和寫入，對整個神經網路的工作方式有了基礎認知。

拿他的模型下來，訓練自己的圖像，形成新的模型。原來神經網路就跟那個，模塊可以長在一塊去，可以切割。Andrej Karpathy做 Tesla FSD，後來又去 OpenAI，大神，AI裡面視覺系大神，他做了一個總結，神經網路是軟體 2.0，可以切割。把一個神經網路規模縮小，他就變笨了。變大，他就變聰明了，但變慢。當時我用過SD，好玩，但真的難用。

**藏师傅：** 對，也是兩種工作流。

**主持人：** 想了解一下您的背景。設計在您的工作裡佔的比例有多少？

**藏师傅：** 現在幫忙做一些SD落地的事情，把SD的流程抽象成C端用戶可用的東西，不用亂七八糟的，一張圖片，甚至打幾個字，甚至直接影片就能轉化成你想要的東西，類似影片特效。

**主持人：** 視頻特效用SD來做。

現在所有的圖像軟體都在往影片方向靠攏。現在在生成式AI領域，還有生成影片。除了剛剛講的，還有 Runway，然後又出來很多類似的。

給大家介紹一下，您用過的市面上幾個主流的，讓大家了解一下。

**藏师傅：** 影片方面，不開源的像 Runway 和 Pika，還有十幾個上線宣傳過的平台，有很多國內的。看著很國外，其實是華人或國內團隊做的。融資比較多、佔據事業比較多的肯定是Runway，做得最久。在圖像模型還不怎樣的時候，就開始做影片模型了，累積的融資和研究性累積最多。

另一個是Pika，最近上線大家都能用到。它把某個風格的效果拉到最好去做宣傳。其他的公司可以借鑒。但實際上它的寫實和交互可能沒有Runway方便。Runway的交互形式比較好，控制也比較強。Pika還有影片編輯。

最近也多了很多，比如 Pixelverse。它的新模型效果也不錯，清晰度和運動幅度達到比較好的平衡。

**主持人：** 聽到您說，我用過 Runway 和 Pika。2023年8月看到有個導演剪輯了一個貓的好萊塢影片，粉色的，當時轉了一下就爆了。通過影片認識了創始人。11月他們開始火的。主要是被一個宣傳影片帶爆的。Runway 比較早用，不是很了解，我挺少用影片編輯軟體的。我覺得現在的影片可以作為圖像生成的補充，讓圖像動起來，做簡單的MV場景，橫移豎移有點動感就到此為止了。如果要做複雜的結構動作太難了。具體的動作目前不太現實，哪怕移動都很費勁。

現在很多剪刀石頭布的動畫，還是用真人在綠幕上錄製動作，然後直接生成。

**藏师傅：** 對，直接去生成。

**主持人：** 剛才說兩個，Runway和Pika。看到您在X上面發用SD版本做的影片，效果特別清晰。

SD是什麼樣的程式或東西做的？

**藏师傅：** SVD是Stability AI發佈的影片上傳模型，有兩個版本。一個只能生成14幀，另一個可以生成25幀。以每秒8幀算，一個1秒多，另一個3秒多一點。它比較強的是清晰度和自然環境的運動，流體、水、火、雲彩和運鏡，運鏡也是靠運氣。

前天 Windows 出了一個方式，可以控制方向，拿箭頭去畫。如果箭頭沒畫到物體上，就變成運鏡方向。如果畫到物體上，就變成物體運動方向。

我覺得它有一定可控性，比現在Runway和Pika還靈活。

**主持人：** SD是 Stable Diffusion 出的。商用方面，他們最近錢不夠，弄了一個會員。新的SDXL Turbo和SVD模型個人用免費，商用要買會員，否則版權有點問題。這東西糊塗帳，往後追模型版權有問題。所以可能他們激進一些，拿這個東西包裝成產品或融資比較好，但現在沒有新動作可惜。因為模型是他們做的，他們肯定最了解。

咱也不排除再做，只是還沒发布。

**主持人：** 這是開源的問題。大家直接用它的模型，可以不用官方提供的服務。Midjourney不開源，用戶飛輪很快。大家在Discord社群共享圖片，可以給他評價。每一次放大、每次選擇，其實就是用戶的偏好，這個數據太重要了。但開源模型只能自用，用自己的模型數據訓練，用戶生成什麼不知道。

**藏师傅：** 對對。

**主持人：** 這對開源項目是個嚴重的問題。如果他可以同時做一個Stable Diffusion 社區版，讓大家用他最新的模型 host 社群，其實也行。但是公司文化和創始人的選擇。

說回來，我們剛好聊回話題，剛開始聊圖像生成的選擇，最主流三個，加上SD。我知道藏师傅日常用最多的是Midjourney，我在X上面看都是Midjourney的提示詞。然後說到影片生成，現在最主流是Runway和新出來的 Pika。還有一些慢慢進入市場。

我自己對生成式AI的看法，兩種媒體生成。我個人是憑使用來評的。

我不是在做測試，我真的是要用。比如聖誕節，我想給公司或產品做海報，首選Midjourney或Dalle，它們能在有限時間內極快生成質量可控的。如果做Research，我好玩可以花很多時間研究，生成很多版本。但真實世界是，我馬上要發海報，必須用一個工具在半小時內做出來。我會優先選Dalle，它畫的還是很醜，感覺沒有Midjourney效果好，但更接近形象化風格。

OpenAI重點不在圖像生成質量，在於表意，把意思表達清楚最重要。所以個人工作流程是，用Dalle把想表達的抽象思維先畫幾張圖出來，它會生成一些描述很有用。再把描述改改修整，放到Midjourney裡面去。通常Midjourney會走形，沒有Dalle畫的那麼想要，但效果好，我就忍一下。差不多這樣，半小時內要出海報我就用這個流程。

SD肯定不可能了。

Adobe Firefly 我沒怎麼用過，沒這兩個工具習慣。它整合到軟件裡面了，我可能打補丁可以用。

想聽聽藏师傅日常有限定時間的任務，會用什麼方式組合？

**藏师傅：** 有多少任務？如果是要生成圖片或影片，給甲方或自己用，要發比較好的，首選 Midjourney 和 Runway、Pika 這些軟件，它們快。如果是工作上的，複雜項目要跟現實生活結合，要跟用戶結合，只能用SD，只能把工作流跑通，部署到後端。

Midjourney 提示詞，很多人來回就那點東西能調能控制。如果要有圖，先describe，把提示詞拿出來。但現在describe用的是V5，所以有時候會走形。V6可能下個月會更好。

另外生成圖片，基本上就是改提示詞，按照既定的權重和命令去試，加詞簡詞。影片的話，一張圖如果Runway出不來，就用Pika試試，Pika 不出來就用Runway試試。現在控制少，只能靠抽卡。

最近都在看SVD，覺得潜力大，但控制難。

**主持人：** 問一下小白問題，SVD是在本地跑還是在雲端買？

**藏师傅：** 我在本地跑。

**主持人：** 效率怎麼樣？

**藏师傅：** 我昨天發的是直接出2K嗎？在ControlNet上用了一些插幀和放大的流程，一個五秒的大概4090要十分鐘左右。

**主持人：** 五秒鐘影片要十分鐘。可控性現在還很弱，只能擺擺鏡頭做個推移。

**藏师傅：** 效果確實好，清晰度上和能動的地方特別好。

**主持人：** 是不是對一些場景的表意做得更好？水紋、紋理、自然的動態會更好？煙霧？

**藏师傅：** 對，煙霧。

**主持人：** 了解。這個模型可能對這種意境的深層次理解訓練更好。

**藏师傅：** 他肯定拋掉了人物那部分，弄不好就不弄了，把其他地方弄好。

**主持人：** 這是個思路，把質量拉滿。這個講完之後，大家對影片生成和圖像生成的優劣勢應該很清楚了，工作流也聽到了。我們倆分享的我有我的小白工作流，我要Google搜 Midjourney 的場景，找到博客抄過來解決問題。

現在有人用ChatGPT做提示詞生成工具，或訓練提示詞的 Fine-tuning 模型。提示詞很重要，能把人類想像的場景充分描繪成提示詞，讓繪圖模型理解，把它繪出來。哪怕現在用不了AI解決問題，我們可以用工程化解決問題，讓工程化工具和生成式AI做更好的結合。藏师傅怎麼看？

**藏师傅：** 本質上是對工作流和交互的打磨。可能我模型沒有那麼好，但我用交互的方式，結合原始使用者的工作方式，提高效果，引導他學會高級技巧。

前段時間我推薦圖像生成工具，它把設計師流程拆解到裡邊。別人生成圖像都是點按鈕生成四張圖再選，它是一個畫板。你在畫板裡生成後，可以自由對整個項目的圖做操作，直接進行下一步操作，比如摳圖和融合。模型質量可能沒那麼好，但思路可用。在畫板裡以項目制的工作流，比Midjourney Web的生產版本更好。

**主持人：** 對，Midjourney會出WebUI嗎？方便大家調整。我

[model=gemini-2.0-flash,0]
